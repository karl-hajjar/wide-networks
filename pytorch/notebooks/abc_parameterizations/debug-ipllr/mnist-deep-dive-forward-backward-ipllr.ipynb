{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "NOTEBOOK_DIR = os.path.dirname(cwd)\n",
    "ROOT = os.path.dirname(os.path.dirname(os.path.dirname(NOTEBOOK_DIR)))\n",
    "\n",
    "FIGURES_DIR = os.path.join(ROOT, 'figures/abc_parameterizations/initialization')\n",
    "CONFIG_PATH = os.path.join(ROOT, 'pytorch/configs/abc_parameterizations', 'fc_ipllr_mnist.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.tools import read_yaml, set_random_seeds\n",
    "from pytorch.configs.base import BaseConfig\n",
    "from pytorch.configs.model import ModelConfig\n",
    "from pytorch.models.abc_params.fully_connected.ipllr import FcIPLLR\n",
    "from pytorch.models.abc_params.fully_connected.muP import FCmuP\n",
    "from pytorch.models.abc_params.fully_connected.ntk import FCNTK\n",
    "from pytorch.models.abc_params.fully_connected.standard_fc_ip import StandardFCIP\n",
    "from utils.data.mnist import load_data\n",
    "from utils.abc_params.debug_ipllr import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load basic configuration and define variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 1\n",
    "SEED = 30\n",
    "L = 6\n",
    "width = 1024\n",
    "n_warmup_steps = 1\n",
    "batch_size = 512\n",
    "base_lr = 0.1\n",
    "n_steps = 50\n",
    "\n",
    "set_random_seeds(SEED)  # set random seed for reproducibility\n",
    "config_dict = read_yaml(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = read_yaml(CONFIG_PATH)\n",
    "\n",
    "input_size = config_dict['architecture']['input_size']\n",
    "\n",
    "config_dict['architecture']['width'] = width\n",
    "config_dict['architecture']['n_layers'] = L + 1\n",
    "config_dict['optimizer']['params']['lr'] = base_lr\n",
    "config_dict['scheduler'] = {'name': 'warmup_switch',\n",
    "                            'params': {'n_warmup_steps': n_warmup_steps,\n",
    "                                       'calibrate_base_lr': True,\n",
    "                                       'default_calibration': False}}\n",
    "        \n",
    "base_model_config = ModelConfig(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data & define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, test_dataset = load_data(download=False, flatten=True)\n",
    "train_data_loader = DataLoader(training_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_batches = list(DataLoader(test_dataset, shuffle=False, batch_size=batch_size))\n",
    "batches = list(train_data_loader)\n",
    "eval_batch = test_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial base lr : [78.5, 47.14387893676758, 62.35230255126953, 61.66318893432617, 69.83497619628906, 80.47408294677734, 24.221656799316406]\n"
     ]
    }
   ],
   "source": [
    "ipllr = FcIPLLR(base_model_config, n_warmup_steps=12, lr_calibration_batches=batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipllr.scheduler.warm_lrs[0] = ipllr.scheduler.warm_lrs[0] * (ipllr.d + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save initial model : t=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipllr_0 = deepcopy(ipllr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model one step : t=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input abs mean in training:  0.6950533986091614\n",
      "loss derivatives for model: tensor([[-0.9000,  0.1000,  0.1000,  ...,  0.1000,  0.1000,  0.1000],\n",
      "        [ 0.1000, -0.9000,  0.1000,  ...,  0.1000,  0.1000,  0.1000],\n",
      "        [ 0.1000,  0.1000,  0.1000,  ...,  0.1000,  0.1000, -0.9000],\n",
      "        ...,\n",
      "        [ 0.1000,  0.1000,  0.1000,  ...,  0.1000,  0.1000, -0.9000],\n",
      "        [ 0.1000,  0.1000,  0.1000,  ...,  0.1000,  0.1000,  0.1000],\n",
      "        [ 0.1000,  0.1000,  0.1000,  ...,  0.1000, -0.9000,  0.1000]])\n",
      "average training loss for model1 : 2.3025991916656494\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = batches[0]\n",
    "train_model_one_step(ipllr, x, y, normalize_first=True)\n",
    "ipllr_1 = deepcopy(ipllr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model for a second step : t=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input abs mean in training:  0.6921874284744263\n",
      "loss derivatives for model: tensor([[ 0.0605,  0.1368,  0.0974,  ...,  0.0852, -0.8384,  0.1191],\n",
      "        [ 0.0461,  0.1547,  0.0935,  ...,  0.0766,  0.1982,  0.1259],\n",
      "        [ 0.0579,  0.1399,  0.0969,  ...,  0.0837,  0.1676,  0.1204],\n",
      "        ...,\n",
      "        [ 0.0572,  0.1407,  0.0967,  ...,  0.0834,  0.1691,  0.1207],\n",
      "        [ 0.0523, -0.8534,  0.0955,  ...,  0.0806,  0.1809,  0.1230],\n",
      "        [ 0.0412,  0.1616,  0.0915,  ...,  0.0730, -0.7863,  0.1281]])\n",
      "average training loss for model1 : 2.35217547416687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = batches[1]\n",
    "train_model_one_step(ipllr, x, y, normalize_first=True)\n",
    "ipllr_2 = deepcopy(ipllr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ipllr.eval()\n",
    "ipllr_0.eval()\n",
    "ipllr_1.eval()\n",
    "ipllr_2.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_scales = ipllr.layer_scales\n",
    "intermediate_layer_keys = [\"layer_{:,}_intermediate\".format(l) for l in range(2, L + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define W0 and b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    W0 = {1: layer_scales[0] * ipllr_0.input_layer.weight.data.detach() / math.sqrt(ipllr_0.d + 1)}\n",
    "    for i, l in enumerate(range(2, L + 1)):\n",
    "        layer = getattr(ipllr_0.intermediate_layers, intermediate_layer_keys[i])\n",
    "        W0[l] = layer_scales[l-1] * layer.weight.data.detach()\n",
    "\n",
    "    W0[L+1] = layer_scales[L] * ipllr_0.output_layer.weight.data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    b0 = layer_scales[0] * ipllr_0.input_layer.bias.data.detach() / math.sqrt(ipllr_0.d + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Delta_W_1 and Delta_b_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Delta_W_1 = {1: layer_scales[0] * (ipllr_1.input_layer.weight.data.detach() -\n",
    "                                       ipllr_0.input_layer.weight.data.detach()) / math.sqrt(ipllr_1.d + 1)}\n",
    "    for i, l in enumerate(range(2, L + 1)):\n",
    "        layer_1 = getattr(ipllr_1.intermediate_layers, intermediate_layer_keys[i])\n",
    "        layer_0 = getattr(ipllr_0.intermediate_layers, intermediate_layer_keys[i])\n",
    "        Delta_W_1[l] = layer_scales[l-1] * (layer_1.weight.data.detach() -\n",
    "                                            layer_0.weight.data.detach())\n",
    "\n",
    "    Delta_W_1[L+1] = layer_scales[L] * (ipllr_1.output_layer.weight.data.detach() -\n",
    "                                        ipllr_0.output_layer.weight.data.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Delta_b_1 = layer_scales[0] * (ipllr_1.input_layer.bias.data.detach() -\n",
    "                                   ipllr_0.input_layer.bias.data.detach()) / math.sqrt(ipllr_1.d + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Delta_W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Delta_W_2 = {1: layer_scales[0] * (ipllr_2.input_layer.weight.data.detach() -\n",
    "                                       ipllr_1.input_layer.weight.data.detach()) / math.sqrt(ipllr_2.d + 1)}\n",
    "    for i, l in enumerate(range(2, L + 1)):\n",
    "        layer_2 = getattr(ipllr_2.intermediate_layers, intermediate_layer_keys[i])\n",
    "        layer_1 = getattr(ipllr_1.intermediate_layers, intermediate_layer_keys[i])\n",
    "        Delta_W_2[l] = layer_scales[l-1] * (layer_2.weight.data.detach() -\n",
    "                                            layer_1.weight.data.detach())\n",
    "\n",
    "    Delta_W_2[L+1] = layer_scales[L] * (ipllr_2.output_layer.weight.data.detach() -\n",
    "                                        ipllr_1.output_layer.weight.data.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Delta_b_2 = layer_scales[0] * (ipllr_2.input_layer.bias.data.detach() -\n",
    "                                   ipllr_1.input_layer.bias.data.detach()) / math.sqrt(ipllr_1.d + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore at step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On examples from the second batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = batches[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x2 = {0: x}\n",
    "    h0 = {1: F.linear(x, W0[1], b0)}\n",
    "    delta_h_1 = {1: F.linear(x, Delta_W_1[1], Delta_b_1)}\n",
    "    delta_h_2 = {1: F.linear(x, Delta_W_2[1], Delta_b_2)}\n",
    "    h1 = {1: layer_scales[0] * ipllr_1.input_layer.forward(x) / math.sqrt(ipllr_1.d + 1)}\n",
    "    h2 = {1: layer_scales[0] * ipllr_2.input_layer.forward(x) / math.sqrt(ipllr_2.d + 1)}\n",
    "    x2[1] = ipllr_2.activation(h2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_allclose(h0[1] + delta_h_1[1], h1[1], rtol=1e-5, atol=1e-5)\n",
    "torch.testing.assert_allclose(h0[1] + delta_h_1[1] + delta_h_2[1], h2[1], rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_1 = delta_h_1[1] * delta_h_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5192)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prod_1 < 0).sum() / prod_1.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4914)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_1[1] < 0).sum() / delta_h_1[1].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9020)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[1] < 0).sum() / delta_h_2[1].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9147)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0[1][0, :].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2897)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1[1][0, :].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3521)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2[1][0, :].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, l in enumerate(range(2, L + 1)):\n",
    "        layer_1 = getattr(ipllr_1.intermediate_layers, intermediate_layer_keys[i])\n",
    "        layer_2 = getattr(ipllr_2.intermediate_layers, intermediate_layer_keys[i])\n",
    "        x = x2[l-1]\n",
    "\n",
    "        h0[l] =  F.linear(x, W0[l])\n",
    "        delta_h_1[l] = F.linear(x, Delta_W_1[l])\n",
    "        delta_h_2[l] = F.linear(x, Delta_W_2[l])\n",
    "        \n",
    "        h1[l] = layer_scales[l-1] * layer_1.forward(x)\n",
    "        h2[l] = layer_scales[l-1] * layer_2.forward(x)\n",
    "        x2[l] = ipllr_2.activation(h2[l])\n",
    "        \n",
    "        torch.testing.assert_allclose(h0[l] + delta_h_1[l], h1[l], rtol=1e-5, atol=1e-5)\n",
    "        torch.testing.assert_allclose(h0[l] + delta_h_1[l] + delta_h_2[l], h2[l], rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = x2[L] \n",
    "    h0[L+1] = F.linear(x, W0[L+1])\n",
    "    delta_h_1[L+1] = F.linear(x, Delta_W_1[L+1])\n",
    "    delta_h_2[L+1] = F.linear(x, Delta_W_2[L+1])\n",
    "    h1[L+1] = layer_scales[L] * ipllr_1.output_layer.forward(x)\n",
    "    h2[L+1] = layer_scales[L] * ipllr_2.output_layer.forward(x)\n",
    "    x2[L+1] = ipllr_2.activation(h2[L+1])\n",
    "                              \n",
    "    torch.testing.assert_allclose(h0[L+1] + delta_h_1[L+1], h1[L+1], rtol=1e-5, atol=1e-5)\n",
    "    torch.testing.assert_allclose(h0[L+1] + delta_h_1[L+1] + delta_h_2[L+1], h2[L+1], rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_1 = delta_h_1[2] * delta_h_2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4599)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prod_1 < 0).sum() / prod_1.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9020)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[1] < 0).sum() / delta_h_2[1].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5661)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[2] < 0).sum() / delta_h_2[2].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4919)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[3] < 0).sum() / delta_h_2[3].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4131)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[4] < 0).sum() / delta_h_2[4].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3740)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[5] < 0).sum() / delta_h_2[4].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3447)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[6] < 0).sum() / delta_h_2[6].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[7] < 0).sum() / delta_h_2[7].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0060, -0.0046,  0.0021,  ...,  0.0014, -0.0062, -0.0022],\n",
       "        [ 0.0009, -0.0007,  0.0003,  ...,  0.0002, -0.0009, -0.0003],\n",
       "        [ 0.0021, -0.0016,  0.0007,  ...,  0.0005, -0.0021, -0.0008],\n",
       "        ...,\n",
       "        [ 0.0009, -0.0007,  0.0003,  ...,  0.0002, -0.0009, -0.0003],\n",
       "        [ 0.0144, -0.0110,  0.0051,  ...,  0.0033, -0.0148, -0.0054],\n",
       "        [ 0.0019, -0.0014,  0.0007,  ...,  0.0004, -0.0019, -0.0007]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_h_2[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h0[7][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_1[7][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_2[7][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h2[7][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 441,  494,  958,   49,  223,  813,   92,  910,  456,  416,   12,   28,\n",
       "         867,  603,  612,  291,  568,  541,   86, 1015,  630,  405,  834,  910,\n",
       "         745,  291,  713,  166,  561,  543,  494,   86,  416,  828,  630,  568,\n",
       "         494,  199,  910,  675,  117,  206,  855,  703,  910,  422,  187,  790,\n",
       "          53,  910,  427,  206,  943,   86,  755,  416,  311,  568,  703,  723,\n",
       "         731,  760,  661,  227,  836, 1015,  206,   11,  227,   42,  206,  813,\n",
       "         910,  942,  745,  227,  510,  403,   86,    9,  117,  291,  209, 1018,\n",
       "          86,  910,  227,  589,   19,  703,  144,  509,  227,  612,   86,  903,\n",
       "         165,  206,   12,  958,  227,  706,  117, 1015,  703,  498,  958,  910,\n",
       "         910,  790,  910,  154,  910,   86,  945,  713,  227,  706,  612,  731,\n",
       "         813,  469,  346,  510,  985,  696,  782,  133,  755,  439,  910,  760,\n",
       "          23,  910,  971,   86,  427,  723,  713,  612,  372,  976,  498,  416,\n",
       "         910,  257,   49,  354,  235,  612,  752,  439,  568,  789,  813,  976,\n",
       "         307,  570,  187,  230,   92,  755, 1015,  439,   11,   51,  524,  976,\n",
       "         514,  756,  976,  723,  439,  892,  892,  958,  199,  760,  441,   90,\n",
       "         439,  683,   92,   86,  910,  612,  942,  416,  223,  150,  330,  350,\n",
       "         117,   59,  684,  818,  976,  142,  227,  199,  346,  117,  427,  870,\n",
       "         427,  176,  612,  755,  882,  894,  942,  985,  570,  910,  755,  654,\n",
       "         319,  206,  755,  813,   86,  708,   86,   92,  570,  117,  416,  755,\n",
       "         790,   74,  206,  416,  910,  813,  291,  760,  416,  494,  452,  813,\n",
       "         699,  230,  752,  855,  745,  954,  206,  766,  494,  708,  630,   86,\n",
       "         975,   86,   12,   92,  828,  494,  942,  708,  790,  199,  612,  416,\n",
       "         703,  291,  817,  612,  703,  368,  187,  616,   64,   86,  330,  612,\n",
       "         478,  154,  971,  531,  696,  958,  346,  745,  870,  291,  942,  752,\n",
       "          42,  185,  153,  144,  855,  743,  227,  416,  274,  766,   86,  869,\n",
       "         958,  227,  795,  836,  154,   86,  199,  660,  176,  703,  227,  752,\n",
       "         291,  615,  227,  957,  385,  789,  291,  439,   92,  976,  223,  568,\n",
       "         854,  469,   86,  903,  945,  612,  206,  960,  760,  550,   86,  439,\n",
       "         291,  612,  910,  227,  731,  789,  912,  758,  369,  954,  144,  755,\n",
       "         612,  713,  987,  910,   12,  556,    9,  910,  869, 1015,  206,  612,\n",
       "         612,  703,  723,  346,  368,  340,   31,  330,  910,  227,  447,  612,\n",
       "         494,  354,   92,  890, 1015,  726,  248,  612,  334,  403,  860,  346,\n",
       "         340,  206,  510,  790,  206,  439,  910,   74,  987,  910,  696,  416,\n",
       "         723,  206,  287,  681,  833,  291,  243,   74,  187,   11,  227,  367,\n",
       "         531,  910,  154,  570,  813,  910,  230,  660,   84,  612,  120,  209,\n",
       "         105,  703,  910,  790,  319,  760,  945,  140,  910,  311,  554,  636,\n",
       "         510,  291,  291,  471,  133,  425,  176,  723,  854,  206,  971,  510,\n",
       "           8,  199,  570,  422,  311,  790,  336,   66,  494,  954,  439,  306,\n",
       "         144,  291,  646,  510,   42,  510,  497,  910,  330,  760,   42,  291,\n",
       "          86,  945,  291,  942,  427,  427,   92,   49,  752,  854,  942,  760,\n",
       "         815,  612,   86,   74,  343,  412,   11,  368,  942,  439,  334,  910,\n",
       "         154,   11,   86,  311,  416,  723,  542,  871,  971,  227,  319,  172,\n",
       "         227,   86,  875,  813,  760,  291,  425,  206])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h0[1][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([541, 524, 602, 541, 541, 541, 356,  68, 524,  68, 524, 524, 602, 541,\n",
       "         25, 602, 790,  68, 524, 524, 541, 541, 541, 541, 156, 524,  68, 524,\n",
       "        541, 541,  68,  68,  68, 156, 524, 790, 524, 541, 541, 602, 839, 156,\n",
       "        541, 541, 541,  68, 524, 541,  68,  68, 541, 156, 541, 524, 541,  25,\n",
       "        356, 524, 541,  68, 602, 602, 356,  68, 524, 261,  68, 524,  68, 541,\n",
       "        199, 524, 541, 524, 839,  68, 602, 839, 541, 524, 790, 541, 602, 602,\n",
       "        541, 541,  68,  68, 541, 541,  68, 541,  68, 541, 541, 541, 524,  68,\n",
       "        524, 541,  68, 602, 156, 261, 541, 541, 541,  68,  68, 541,  68, 541,\n",
       "        541, 524, 541,  68,  68, 602, 524, 541, 524, 709, 602, 524, 541, 524,\n",
       "        302, 541, 541, 156,  68, 602, 541,  68, 156,  68, 337,  68, 541, 528,\n",
       "        528, 541,  68, 156,  68, 261, 524, 839,  68, 524,  68, 790, 602, 156,\n",
       "        541, 541,  68, 356, 790, 524, 541, 541, 524, 156, 156, 541, 120, 541,\n",
       "        524, 611, 541,  68, 541, 541, 541, 602, 541, 602, 524, 524, 790,  68,\n",
       "        302, 541, 541, 541, 541, 156, 541, 541, 790, 602, 156, 541, 541, 524,\n",
       "        541, 524,  68, 602, 541, 839, 611, 541, 524, 541, 524, 541, 541, 524,\n",
       "        541, 541, 602, 541, 541, 611, 935,  68, 350, 541, 541, 602, 228, 541,\n",
       "        356, 839, 524, 541, 524, 611, 524,  68,  68, 541, 524, 120, 541, 524,\n",
       "        228, 524, 228, 302, 541, 524, 935, 524, 541, 302, 136, 156, 541, 541,\n",
       "        524, 356, 524, 356, 541, 524, 541, 602, 602, 228, 524,  68, 752, 524,\n",
       "        524,  68, 541, 524, 528, 602, 541, 541, 790, 541, 541, 541, 602,  25,\n",
       "        790, 524, 602, 524, 524, 602, 541, 541, 541, 541, 541, 524, 524, 323,\n",
       "         68, 524, 541, 524, 228, 667, 541,  68, 524, 602, 541, 541, 541, 524,\n",
       "        541, 541,  68, 541, 524, 524,  68, 524, 302, 302, 524, 156, 541, 524,\n",
       "        524, 790, 541, 524, 156, 120, 541, 524,  68, 602, 602, 524, 541, 790,\n",
       "        602, 524, 541,  68, 602, 337, 602, 302, 602, 524, 541, 541, 541, 541,\n",
       "        524, 120, 524, 602,  68, 541, 667, 302,  68, 602, 528, 541,  68, 602,\n",
       "        524, 356, 541, 524, 541,  68, 541, 337, 156, 524, 541, 524, 302, 350,\n",
       "        541, 524,  68, 524, 524, 602, 541, 611, 602, 199, 853, 790,  68,  68,\n",
       "        524, 541, 156, 528,  68, 541, 524,  68, 350, 524, 541,  68, 541, 156,\n",
       "         68, 524, 261, 541, 541, 524, 356, 541, 541, 611, 602, 524,  68, 709,\n",
       "        541, 541, 541, 602, 156, 790, 541, 337,  68,  68, 528, 602, 524, 524,\n",
       "        602, 524, 611,  68, 541,  68, 541, 120,  68,  68, 790, 524, 541, 524,\n",
       "        541, 541, 524, 337,  68, 356,  68, 156, 602, 524, 611, 541, 541, 524,\n",
       "        156, 541, 156, 790, 541, 541, 524, 541, 602, 120, 228, 199, 524,  68,\n",
       "        524, 302, 541, 528, 853, 524, 541,  68, 541, 524, 524, 524, 120, 156,\n",
       "         68, 541, 524, 156, 524,  68, 524,  68, 541, 524, 602,  68, 524, 541,\n",
       "         68, 524, 602, 541, 541, 602,  68, 541])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_1[1][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  49,  104,   49,  336,  336,  336,  104,   49,  104,   49,  104,  336,\n",
       "          49,  336,   49,   49,  104,   49,  104,  694,   49,  104,  336,  336,\n",
       "         104,  104,   49,  104,  104,  104,   49,   49,   49,   49,  336,  104,\n",
       "         336,   49,   49,   49, 1013,   49,  683,  336,   49,  336,  104,   49,\n",
       "          49,   49,   49,   49,  336,  104,  336,   49,  104,  104,  336,  227,\n",
       "          49,   49,  336,  227,   49,  336,   49,  104,  227,  336,   49,  336,\n",
       "          49,  336,  104,   49,   49,  104,   49,  104,  104,   49,   49,   49,\n",
       "         336,   49,   49,  227,   49,  336,   49,  104,  227,   49,  336,   49,\n",
       "         104,   49,   49,   49,  227,   49,  104,  104,  336,   49,  336,   49,\n",
       "          49,   49,  227,  104,  336,  104,  336,   49,   49,   49,  104,   49,\n",
       "         336,   49,   49,  336,  336,  104,  104,   49,   49,  104,  227,   49,\n",
       "         336,   49,  104,   49,   49,   49,   49,  104,  813,  104,   49,   49,\n",
       "          49,   49,  336,   49,   49,  104,   49,  104,   49,  104,  336,  336,\n",
       "         336,  104,   49,  104,  104,  336,   49,   49,  104,   49,  336,   49,\n",
       "         694,   49,  683,  227,  104,   49,  336,   49,   49,   49,  104,  336,\n",
       "          49,   49,  104,   49,   49,   49,  336,   49,   49,  336,   49,   49,\n",
       "         104,   49,  336,  336,   49,   49,  227,   49,   49, 1013,   49,  336,\n",
       "         694,  683,  104,  336,  336,  104,   49,  336,   49,  336,  336,  683,\n",
       "          49,   49,  336,  336,   49,   49,   49,  104,  104,  104,   49,  336,\n",
       "         104,   49,   49,   49,   49,   49,   49,   49,  336,   49,   49,  104,\n",
       "          49,  104,  336,  683,  104,  336,   49,  104,   49,  104,   49,   49,\n",
       "          49,  104,   49,  104,  336,  104,   49,   49,   49,   49,  336,   49,\n",
       "         336,  104,  336,   49,  336,   49,   49,   49,  104,   49,  104,   49,\n",
       "          49,  104,   49,  336,  104,  104,   49,  104,  104,   49,   49,  336,\n",
       "         336,   49,   49,  104,  104,   49,   49,   49,  104,  104,   49,  336,\n",
       "          49,   49,  104,   49,  104,   49,   49,  336,  336,  336,   49,  336,\n",
       "         104,  104,  227,   49, 1013, 1013,   49,   49,  336,  336,  104,   49,\n",
       "         104,  336,   49,   49,  336,  104,   49,   49,   49, 1013,   49,   49,\n",
       "          49,  104,  336,   49,   49,   49,   49,  104,   49,  336,   49,  336,\n",
       "          49,   49,  104,   49,  104,   49,   49,  336,  336,  104,   49,   49,\n",
       "          49,   49,   49,   49,  104,  336,  336,  104,  336,   49,  336,   49,\n",
       "         104,  104,  336,  104,  104,  104,  104,   49,  227,  104,  104,   49,\n",
       "         336,   49,   49,   49,   49,   49,   49,   49,  336,  336,  104,   49,\n",
       "         227,   49,  336,   49,  336,   49,   49,  227,  104,  104,  227,   49,\n",
       "         336,  336,  104,  104,  104,  336,  104,   49,   49,  104,  227,   49,\n",
       "         104,  336,  336,   49,  104,   49,  336,   49,   49,  227,   49,   49,\n",
       "          49,   49,   49,   49,   49,   49,  104,   49,  104,   49,   49,   49,\n",
       "         104,  104,   49,  636,  336,  104,  636,   49,  683,  336,   49,   49,\n",
       "          49,   49,   49,   49,  336,   49,   49,  336,  104,   49,  336,   49,\n",
       "         336,  104,   49,   49,   49,   49,  104,  336,  104,  104,  336,  104,\n",
       "         104,  104,   49,  227,  336,   49,  336,   49,  336,  813,  227,  336,\n",
       "         694,   49,  683,  227,   49,   49,  683,  104,   49,   49,  104,   49,\n",
       "         227,   49,  104,  104,   49,   49,  227,   49])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_2[1][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 925,  156,  404,   49,  350,  220,  302,  120,  891,  120,  220,  417,\n",
       "         962,  220,  528,  274,  932,   68,  648,  667,   39, 1000,  220,  515,\n",
       "         547,  165,  801,  656,  482,  515,   68,  294,   68,  853,  350,  791,\n",
       "         420,  220,  407,  404,  547,  853,  515,   68,  515,  660,  220,  220,\n",
       "         120,  971,   39,  853,  567,  420,  220,  803,  220,  997,  231,   68,\n",
       "         294,   76,  356,  227,  997,  779,  993,  853,   68,  220,   57,  460,\n",
       "         220,  620,  482,   68,  962,  667,  515,  527,   70,  750,  183,  274,\n",
       "         292,  220,  227,  971,  925,  482,  251,  220,  120,  896,  407,  709,\n",
       "         165,  227, 1006,  515,   68,  361,   70,  361,  482,  783,  515,  120,\n",
       "          68,  220,   68,  925,  515,  749,  667,   68,  227,  274,  220,  220,\n",
       "         292,  220,   57,  228,  667,  165,  782,  274,  220,  482,   68,   76,\n",
       "         331,  611,  932,  750,  611,  120,  361,  853,  372,  925,  120,  853,\n",
       "         578,  801,   49,  979,  361,  791,  120,   63,  274,  853,  220,  925,\n",
       "         417,  667,   41,   70,  515,  667,   68,  880,   56,  750,  251,  220,\n",
       "         560,  618,  220,   68,  302,  515,  302,  220,  220,  220,  791,  460,\n",
       "          70,  227,  356,  515,   68,  681,  709,   68,  460,  925,  645,   57,\n",
       "        1000,  515,  220,  648,   57,  801,  120,   76,  274,  547,  611,  274,\n",
       "         791,  220,  220,  667,  220,  791,  220,  228,  274,  667,  709,  654,\n",
       "         750,  611,  482,  515,  218,  220,  228,  667,  220,  880,  156,  220,\n",
       "         749,   68,  156,  120,  750,  515,  220,  816,  220,  528,  228,  220,\n",
       "         779,  667,  709,  220,  542,  350,   39,  220,  853,  317,  611,  795,\n",
       "         274,  228,  853,  274,  220,  997,  220,  220,  274,  529,  853,  120,\n",
       "         833, 1006,   94,   68,  482,  801,  853,   57,  667,  274,   70,  925,\n",
       "         478,  925,  971,  471,  749,  220,  361,  407,  853,  361,  648,  482,\n",
       "         251,  220,  541,  648,  791, 1000,  227,  791,  274,  648,  228,  667,\n",
       "         515,  227,  795,  947,  515,  220,  515,  660,  482,  482,  227,  460,\n",
       "         231,  575,  120,  791,    2,  482,  404,   63,  925,  925,  791,  245,\n",
       "         541, 1013,  220,  906,  515,  667,  750,  274,  220,  891,  220,  880,\n",
       "         220,  220,  925,   68,  361,  611,   76,  667,  361,  350,  308,  220,\n",
       "         220,   68,  350,  833,  791,  482,   68,  407,  667,  515,   57,  294,\n",
       "         220,  567,   68,  274,  801,  779,   49,  274,   68,   68,  667,   68,\n",
       "         251,  749,  667,  220,  156,  925,  220,  515,  120,  667,  925,   57,\n",
       "         220,  206,  750,  750,  801,    2,  120,  120,  425,  667,  220,  853,\n",
       "          68,   57,  218,  120,  833,  160,  515,  120,  667,  482,  227,  896,\n",
       "         667,  220,  925,  803,  350,  482,  274,  752,  220,  220,  120,  131,\n",
       "         220,  220,  220,  260, 1006,  156,  541,  611,   68,  971,  853,  218,\n",
       "         853,  853,  404,  425,  133,   68,  220,  611,  220,   57,  120,   68,\n",
       "         648,  165,  220,  618,  220,  274,  611,   57,  611,  667,  642,   70,\n",
       "         220,  156,  611,  168,  482,  791,  547,  482,  749,  732,  515,  274,\n",
       "         648,  220,  361,  681,   39,   57,  220,   49,  220,  302,  220,  853,\n",
       "         853,  156,  220,   68,  515,  220,   30,  997,  660,   70,  120,  220,\n",
       "         477,   32,  648,   68,  853,  723,  220,   76,  795,  120,  667,  515,\n",
       "         971,  853,  875,  220,  515,  274,   68,  228])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h2[1][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
       "        582, 582, 582, 582, 582, 582, 582, 582])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h0[6][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_1[6][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215, 215,\n",
       "        215, 215, 215, 215, 215, 215, 215, 215])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_2[6][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545, 545,\n",
       "        545, 545, 545, 545, 545, 545, 545, 545])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h2[6][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 517,  951,   25,  661,  531,   42,  928,  902,  924,  902,  798,  375,\n",
       "         707,  611,  902,  960,  611,  778,  418,  974,  882,  843,  611,   42,\n",
       "         306,  980,  902,  148,  611,  886,  491,  902,  902,  491,  377,  749,\n",
       "         418,  632,  517,  742,  850,  491,  246,  843,  843,  902,  193,  913,\n",
       "         902,  902,  902,  882,  778, 1016,  246,  902,  825,  924,  246,  446,\n",
       "         517,  843,  611,  902,  989,  246,  902,  392,  797,  193,  902,  878,\n",
       "         244,  902,  306,  797,  242,  596,  186,  519,  306,  902,  743,  843,\n",
       "          42,  843,  778,  446,  895,  611,  797,  424,  446,  913,  895,  196,\n",
       "         121,  491,  902,  797,  446,  350,  306,  950,  611,  997,  611,  446,\n",
       "         882,   42,  838,  400,  334,  611,  611,  491,  882,  960,  193,   42,\n",
       "         895,  679,  196,  517,  611,  974,  428,  902,  916,  306,  838,  740,\n",
       "         531,  882,  227,  902,  902,  902,  997,  340,  381,  974,  601,  491,\n",
       "         902,  491,  446,  381,  679,  193,  902,  614,  611,  763,  895,  843,\n",
       "         902,   42,  440,  611,  886,  611,  994,  434,  627,  902,  994,  843,\n",
       "          42,  902,  769,  446,   42,  517,  246,  193,  533,  350,  974,  564,\n",
       "         343,  882,  789,  970,  838,  893,  418,   86,  978,   42,  176,  196,\n",
       "         163,  517,  246,  794,  902,  601,  882,  960,  611,  238,  902,   42,\n",
       "         842,  246,  965,  611,   42,  476,   42,  193,  511,  843,  334,  601,\n",
       "         491,  882,  246,  611,  902,  654,  193,  611,  916,  519,  997,  246,\n",
       "         611,  902,  611,  902,  902,   42,  902,  778,  611,  994,  193,  611,\n",
       "         902,  271,  902,  994,  850,  878,  902,  886,  170,   47,  902,  895,\n",
       "         994,  537,  902, 1007,   43,  849,  895,  994,  817,  196,  403,  902,\n",
       "         893,  418,   42,  843,  246,  992,  491,  196,  792,  244,  611,  997,\n",
       "         454,  886,  310,  193,  974,   42,  960,  381,  751,  994,   42,  246,\n",
       "         246,  843,  196,  968,  590,  489,  902,  385,  924,  528,  992,  193,\n",
       "         797,  882,  246,  305,  886,   42,  610,  288,  246,  246,  902,  924,\n",
       "         418,  340,  446,  150,  381,  194,  825,  397,  661,  661,   35,  403,\n",
       "         825,  596,  193,   39,  635,  974,  902,  351,  611,  924,  895,  519,\n",
       "         350,   61,  504,  882,  960,  902,  843,  381,  997,  902,  893,  334,\n",
       "         902,  610,  843,  902,  974,  611,  491,  843,  531,  995,  902,  843,\n",
       "         519,  902,  902,  913,  229,  902,  246,  611,  334,  882,  611,  997,\n",
       "         827,  749,  246,  246,  381,  246,  611,  564,  882,  692,  519,  679,\n",
       "          42,  902,  843,  902,  902,  306,  446,  950,  661,   42,  379,  997,\n",
       "         446,  902,  112,  893,  537,  150,  400,  446,  611,  197,  882,  809,\n",
       "         606,  246,  974, 1007,  611,  611,  611,  902,  950,  401,  882,  960,\n",
       "         400,   42,  661,  654,  634,  368,  403,  902,  902,  446,  997,  610,\n",
       "         902,  368,  517,  968,  902,  902,  246,  902,   42,  902,  902,  601,\n",
       "         310,  974,  902,  902,  246,  611,  601,  994,  166,  531,  446,  381,\n",
       "         196,  838,  902,  902,  246,  858,  872,  246,  611,  545,  611,  611,\n",
       "         758,  611,  196,  893,  196,  902,  194,  902,  517,  843,  246,  924,\n",
       "         194,  950,  123,  902,  517,  827,  176,  476,  778,  310,  446,  611,\n",
       "          42,  778,  994,  882,  997,  902,  843,  574,  446,  797,  619,  740,\n",
       "         882,  902,  611,  611,  886,  968,  902,   42])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h0[2][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26953125"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unique().numel() / b.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([797, 814, 797, 797, 893, 895, 895, 797, 895, 797, 895, 797, 577, 895,\n",
       "        797, 797, 340, 797, 797, 895, 797, 340, 893, 797, 340, 797, 797, 814,\n",
       "        340, 895, 797, 797, 797, 797, 797, 340, 797, 797, 797, 577, 340, 797,\n",
       "        797, 797, 797, 797, 340, 893, 797, 797, 797, 797, 797, 340, 893, 797,\n",
       "        797, 340, 797, 797, 797, 797, 797, 797, 895, 797, 797, 340, 797, 797,\n",
       "        797, 895, 797, 797, 340, 797, 797, 340, 797, 814, 340, 797, 797, 797,\n",
       "        895, 797, 797, 797, 797, 893, 797, 340, 797, 340, 797, 797, 797, 797,\n",
       "        797, 895, 797, 797, 340, 797, 797, 797, 797, 797, 797, 797, 797, 895,\n",
       "        797, 340, 895, 797, 797, 797, 340, 797, 895, 797, 797, 797, 893, 814,\n",
       "        895, 797, 895, 340, 797, 797, 895, 797, 340, 797, 797, 797, 797, 203,\n",
       "        340, 895, 797, 797, 797, 797, 797, 340, 797, 895, 797, 340, 895, 814,\n",
       "        797, 895, 797, 895, 340, 340, 895, 893, 797, 340, 340, 797, 797, 797,\n",
       "        895, 797, 895, 797, 895, 797, 797, 797, 895, 797, 895, 797, 340, 797,\n",
       "        895, 797, 797, 797, 797, 797, 797, 797, 340, 797, 340, 797, 340, 895,\n",
       "        797, 797, 797, 797, 895, 340, 797, 895, 895, 895, 340, 893, 895, 895,\n",
       "        895, 797, 895, 797, 797, 797, 797, 797, 797, 895, 797, 797, 797, 895,\n",
       "        895, 340, 797, 893, 340, 797, 797, 797, 797, 797, 797, 797, 895, 797,\n",
       "        797, 895, 797, 340, 797, 797, 814, 797, 797, 895, 814, 340, 797, 797,\n",
       "        895, 797, 797, 895, 797, 340, 797, 797, 797, 797, 797, 797, 797, 797,\n",
       "        797, 797, 893, 814, 814, 797, 340, 797, 340, 797, 895, 895, 577, 797,\n",
       "        340, 895, 797, 814, 814, 895, 797, 797, 797, 797, 797, 895, 895, 340,\n",
       "        797, 797, 895, 895, 797, 893, 895, 797, 797, 797, 895, 797, 797, 797,\n",
       "        893, 893, 797, 797, 797, 203, 797, 895, 340, 340, 895, 340, 797, 895,\n",
       "        340, 895, 895, 895, 797, 797, 895, 340, 797, 895, 340, 340, 797, 340,\n",
       "        797, 340, 893, 797, 797, 797, 895, 340, 797, 797, 797, 797, 797, 797,\n",
       "        895, 797, 814, 340, 797, 797, 895, 895, 797, 340, 340, 797, 797, 895,\n",
       "        814, 797, 797, 340, 797, 797, 895, 797, 797, 340, 797, 797, 340, 797,\n",
       "        340, 797, 797, 895, 340, 797, 895, 797, 797, 797, 797, 340, 797, 797,\n",
       "        797, 893, 340, 814, 797, 797, 814, 797, 797, 814, 797, 797, 340, 340,\n",
       "        797, 340, 893, 893, 340, 895, 895, 893, 340, 797, 797, 340, 797, 797,\n",
       "        895, 797, 893, 340, 797, 340, 797, 797, 797, 797, 814, 577, 797, 203,\n",
       "        797, 797, 797, 797, 895, 797, 895, 797, 797, 797, 340, 797, 797, 797,\n",
       "        797, 340, 797, 797, 797, 797, 797, 340, 797, 797, 797, 797, 895, 797,\n",
       "        340, 893, 340, 340, 895, 895, 797, 895, 797, 797, 797, 797, 895, 797,\n",
       "        797, 895, 797, 340, 203, 797, 895, 797, 797, 895, 340, 814, 797, 340,\n",
       "        797, 893, 895, 577, 797, 797, 797, 797, 895, 797, 797, 797, 814, 895,\n",
       "        797, 797, 340, 895, 895, 893, 797, 797])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_1[2][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 560, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 560, 523, 523, 523,\n",
       "        560, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 560, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 560,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 560, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 560, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 560, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 560, 523, 523, 523, 523, 523, 523, 523, 523, 560, 523, 523,\n",
       "        560, 523, 560, 523, 523, 523, 523, 523, 560, 523, 560, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 560, 523, 523, 523, 523,\n",
       "        523, 560, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 560, 523, 523, 523, 523, 560, 523, 523, 523,\n",
       "        523, 560, 523, 523, 560, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 560, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 560, 523, 560, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 560, 523, 523, 523, 523, 523, 523,\n",
       "        523, 560, 560, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 560, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 560, 560, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523, 523,\n",
       "        523, 523, 523, 523, 523, 523, 523, 560, 523, 523, 523, 523, 523, 523,\n",
       "        523, 560, 523, 523, 523, 523, 523, 523])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_2[2][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([797, 814, 797, 797, 895, 895, 895, 797, 895, 797, 895, 797, 577, 895,\n",
       "        902, 797, 340, 797, 797, 895, 797, 895, 893, 797, 340, 797, 797, 797,\n",
       "        340, 895, 797, 797, 797, 797, 797, 340, 797, 797, 797, 577, 340, 797,\n",
       "        797, 797, 797, 797, 340, 893, 797, 797, 797, 797, 797, 340, 893, 797,\n",
       "        797, 340, 797, 797, 797, 797, 797, 797, 895, 797, 797, 340, 797, 797,\n",
       "        797, 895, 797, 797, 340, 797, 797, 340, 797, 203, 340, 797, 797, 797,\n",
       "        895, 797, 797, 797, 797, 893, 797, 895, 797, 340, 797, 797, 797, 797,\n",
       "        797, 895, 797, 797, 340, 797, 893, 797, 797, 797, 797, 797, 797, 895,\n",
       "        797, 340, 895, 797, 797, 797, 340, 797, 895, 797, 797, 797, 893, 895,\n",
       "        895, 797, 895, 340, 797, 797, 895, 797, 340, 797, 797, 797, 797, 203,\n",
       "        340, 895, 797, 797, 797, 797, 797, 340, 797, 340, 797, 340, 895, 814,\n",
       "        895, 895, 797, 895, 340, 340, 895, 893, 797, 340, 340, 797, 797, 797,\n",
       "        895, 797, 895, 797, 895, 797, 797, 797, 895, 797, 895, 797, 340, 797,\n",
       "        895, 797, 797, 797, 797, 797, 797, 797, 340, 797, 340, 797, 340, 895,\n",
       "        797, 797, 797, 797, 895, 340, 797, 895, 895, 895, 340, 893, 895, 403,\n",
       "        895, 797, 895, 797, 797, 797, 797, 797, 893, 895, 797, 797, 797, 895,\n",
       "        895, 340, 797, 893, 340, 797, 797, 797, 797, 797, 797, 797, 895, 797,\n",
       "        797, 895, 797, 340, 797, 893, 814, 797, 797, 895, 814, 203, 797, 797,\n",
       "        895, 895, 797, 895, 797, 340, 797, 797, 797, 797, 797, 797, 797, 797,\n",
       "        797, 797, 893, 814, 814, 797, 340, 797, 340, 797, 895, 895, 577, 797,\n",
       "        340, 895, 797, 814, 895, 895, 797, 797, 797, 797, 797, 895, 895, 340,\n",
       "        797, 797, 895, 895, 797, 895, 895, 797, 797, 797, 895, 797, 797, 797,\n",
       "        893, 893, 797, 797, 797, 340, 797, 814, 340, 340, 895, 340, 797, 895,\n",
       "        340, 895, 895, 895, 797, 797, 895, 340, 797, 895, 340, 340, 797, 340,\n",
       "        797, 340, 797, 797, 797, 797, 895, 340, 797, 797, 797, 797, 797, 797,\n",
       "        895, 797, 814, 340, 797, 797, 895, 895, 797, 340, 340, 797, 797, 895,\n",
       "        797, 797, 797, 340, 797, 797, 895, 797, 797, 340, 797, 797, 340, 797,\n",
       "        340, 797, 797, 340, 895, 797, 895, 797, 797, 797, 797, 340, 797, 797,\n",
       "        797, 893, 340, 902, 797, 797, 340, 797, 797, 814, 797, 797, 340, 340,\n",
       "        797, 340, 893, 893, 895, 895, 895, 893, 340, 797, 797, 340, 797, 797,\n",
       "        895, 797, 893, 340, 797, 340, 797, 797, 797, 797, 814, 797, 797, 203,\n",
       "        797, 797, 797, 797, 797, 797, 895, 797, 797, 797, 340, 797, 797, 797,\n",
       "        797, 340, 797, 797, 797, 797, 797, 340, 797, 797, 797, 797, 895, 797,\n",
       "        340, 893, 340, 340, 895, 895, 797, 895, 797, 797, 797, 797, 895, 797,\n",
       "        895, 895, 797, 340, 203, 797, 895, 797, 797, 895, 340, 814, 797, 340,\n",
       "        797, 893, 895, 577, 797, 797, 797, 797, 797, 797, 797, 797, 814, 895,\n",
       "        797, 797, 340, 895, 895, 895, 797, 797])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h2[2][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unique().numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h0</th>\n",
       "      <th>delta_h_1</th>\n",
       "      <th>delta_h_2</th>\n",
       "      <th>h1</th>\n",
       "      <th>h2</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>68</td>\n",
       "      <td>105</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        h0 delta_h_1 delta_h_2  h1   h2  batch_size\n",
       "layer                                              \n",
       "1      171        25         9  68  105         512\n",
       "2      138         7         2   9    9         512\n",
       "3       15         1         2   1    1         512\n",
       "4        2         1         1   1    1         512\n",
       "5        1         1         1   1    1         512\n",
       "6        1         1         1   1    1         512\n",
       "7        1         1         1   1    1         512"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['h0', 'delta_h_1', 'delta_h_2', 'h1', 'h2']\n",
    "df = pd.DataFrame(columns=columns, index=range(1, L+2))\n",
    "df.index.name = 'layer'\n",
    "\n",
    "for l in df.index:\n",
    "    maxes = dict()\n",
    "    \n",
    "    _, maxes['h0'] = torch.max(h0[l] , dim=1)\n",
    "    _, maxes['delta_h_1'] = torch.max(delta_h_1[l] , dim=1)\n",
    "    _, maxes['delta_h_2'] = torch.max(delta_h_2[l] , dim=1)\n",
    "    _, maxes['h1'] = torch.max(h1[l] , dim=1)\n",
    "    _, maxes['h2'] = torch.max(h2[l] , dim=1)\n",
    "\n",
    "    df.loc[l, columns] = [maxes[key].unique().numel() for key in columns]\n",
    "    \n",
    "df.loc[:, 'batch_size'] = batch_size\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999744"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.00195312 * 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h0[7] , dim=1)\n",
    "b.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001953125"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unique().numel() / 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h0</th>\n",
       "      <th>delta_h_1</th>\n",
       "      <th>delta_h_2</th>\n",
       "      <th>h1</th>\n",
       "      <th>h2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.91465</td>\n",
       "      <td>0.899165</td>\n",
       "      <td>0.539691</td>\n",
       "      <td>1.2897</td>\n",
       "      <td>1.35211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035248</td>\n",
       "      <td>0.642963</td>\n",
       "      <td>0.00274383</td>\n",
       "      <td>0.655749</td>\n",
       "      <td>0.654193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0247807</td>\n",
       "      <td>0.649087</td>\n",
       "      <td>0.00478905</td>\n",
       "      <td>0.660623</td>\n",
       "      <td>0.656214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0248684</td>\n",
       "      <td>0.636594</td>\n",
       "      <td>0.00525915</td>\n",
       "      <td>0.649044</td>\n",
       "      <td>0.644057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.023723</td>\n",
       "      <td>0.618592</td>\n",
       "      <td>0.00547487</td>\n",
       "      <td>0.629891</td>\n",
       "      <td>0.62506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0236104</td>\n",
       "      <td>0.598841</td>\n",
       "      <td>0.00673273</td>\n",
       "      <td>0.608588</td>\n",
       "      <td>0.604272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0619903</td>\n",
       "      <td>0.0578427</td>\n",
       "      <td>0.00260525</td>\n",
       "      <td>0.117393</td>\n",
       "      <td>0.115223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              h0  delta_h_1   delta_h_2        h1        h2\n",
       "layer                                                      \n",
       "1        0.91465   0.899165    0.539691    1.2897   1.35211\n",
       "2       0.035248   0.642963  0.00274383  0.655749  0.654193\n",
       "3      0.0247807   0.649087  0.00478905  0.660623  0.656214\n",
       "4      0.0248684   0.636594  0.00525915  0.649044  0.644057\n",
       "5       0.023723   0.618592  0.00547487  0.629891   0.62506\n",
       "6      0.0236104   0.598841  0.00673273  0.608588  0.604272\n",
       "7      0.0619903  0.0578427  0.00260525  0.117393  0.115223"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['h0', 'delta_h_1', 'delta_h_2', 'h1', 'h2']\n",
    "df = pd.DataFrame(columns=columns, index=range(1, L+2))\n",
    "df.index.name = 'layer'\n",
    "for l in df.index:\n",
    "    df.loc[l, columns] = [h0[l][0, :].abs().mean().item(), delta_h_1[l][0, :].abs().mean().item(), \n",
    "                          delta_h_2[l][0, :].abs().mean().item(), h1[l][0, :].abs().mean().item(),  \n",
    "                          h2[l][0, :].abs().mean().item()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4443)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[2][0, :] * delta_h_2[2][0, :]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4727)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[3][0, :] * delta_h_2[3][0, :]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4102)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[4][0, :] * delta_h_2[4][0, :]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3735)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[5] * delta_h_2[5]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3438)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[6] * delta_h_2[6]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8000)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[7][132, :] * delta_h_2[7][132, :]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6000)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h1[7][1, :] < 0).sum() / h1[7][1, :].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
