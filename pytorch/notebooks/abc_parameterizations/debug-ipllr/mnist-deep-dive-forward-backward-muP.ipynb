{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "NOTEBOOK_DIR = os.path.dirname(cwd)\n",
    "ROOT = os.path.dirname(os.path.dirname(os.path.dirname(NOTEBOOK_DIR)))\n",
    "\n",
    "FIGURES_DIR = os.path.join(ROOT, 'figures/abc_parameterizations/initialization')\n",
    "CONFIG_PATH = os.path.join(ROOT, 'pytorch/configs/abc_parameterizations', 'fc_ipllr_mnist.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.tools import read_yaml, set_random_seeds\n",
    "from pytorch.configs.base import BaseConfig\n",
    "from pytorch.configs.model import ModelConfig\n",
    "from pytorch.models.abc_params.fully_connected.ipllr import FcIPLLR\n",
    "from pytorch.models.abc_params.fully_connected.muP import FCmuP\n",
    "from pytorch.models.abc_params.fully_connected.ntk import FCNTK\n",
    "from pytorch.models.abc_params.fully_connected.standard_fc_ip import StandardFCIP\n",
    "from utils.data.mnist import load_data\n",
    "from utils.abc_params.debug_ipllr import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load basic configuration and define variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 1\n",
    "SEED = 30\n",
    "L = 6\n",
    "width = 1024\n",
    "n_warmup_steps = 1\n",
    "batch_size = 512\n",
    "base_lr = 0.1\n",
    "n_steps = 50\n",
    "\n",
    "set_random_seeds(SEED)  # set random seed for reproducibility\n",
    "config_dict = read_yaml(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = read_yaml(CONFIG_PATH)\n",
    "\n",
    "input_size = config_dict['architecture']['input_size']\n",
    "\n",
    "config_dict['architecture']['width'] = width\n",
    "config_dict['architecture']['n_layers'] = L + 1\n",
    "config_dict['optimizer']['params']['lr'] = base_lr\n",
    "        \n",
    "base_model_config = ModelConfig(config_dict)\n",
    "base_model_config.scheduler = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data & define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, test_dataset = load_data(download=False, flatten=True)\n",
    "train_data_loader = DataLoader(training_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_batches = list(DataLoader(test_dataset, shuffle=False, batch_size=batch_size))\n",
    "batches = list(train_data_loader)\n",
    "eval_batch = test_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "muP = FCmuP(base_model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, param_group in enumerate(muP.optimizer.param_groups):\n",
    "    if l == 0:\n",
    "        param_group['lr'] = param_group['lr'] * (muP.d + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save initial model : t=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "muP_0 = deepcopy(muP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model one step : t=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input abs mean in training:  0.6950533986091614\n",
      "loss derivatives for model: tensor([[-0.8969,  0.1024,  0.1054,  ...,  0.0975,  0.0981,  0.0973],\n",
      "        [ 0.1020, -0.8974,  0.1068,  ...,  0.0999,  0.1001,  0.0976],\n",
      "        [ 0.1028,  0.1035,  0.1075,  ...,  0.0969,  0.1011, -0.9035],\n",
      "        ...,\n",
      "        [ 0.1007,  0.1015,  0.1078,  ...,  0.0990,  0.0991, -0.9043],\n",
      "        [ 0.1040,  0.1026,  0.1052,  ...,  0.0975,  0.1001,  0.0976],\n",
      "        [ 0.1053,  0.1027,  0.1094,  ...,  0.0963, -0.8984,  0.0981]])\n",
      "average training loss for model1 : 2.303544521331787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = batches[0]\n",
    "train_model_one_step(muP, x, y, normalize_first=True)\n",
    "muP_1 = deepcopy(muP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model for a second step : t=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input abs mean in training:  0.6921874284744263\n",
      "loss derivatives for model: tensor([[ 0.0947,  0.1087,  0.1084,  ...,  0.0948, -0.8999,  0.0992],\n",
      "        [ 0.1012,  0.1073,  0.1296,  ...,  0.0830,  0.1024,  0.0911],\n",
      "        [ 0.1007,  0.1040,  0.1072,  ...,  0.0943,  0.1014,  0.0981],\n",
      "        ...,\n",
      "        [ 0.1057,  0.0997,  0.1064,  ...,  0.0922,  0.1043,  0.0990],\n",
      "        [ 0.0970, -0.8785,  0.1138,  ...,  0.0948,  0.0976,  0.0933],\n",
      "        [ 0.1000,  0.1067,  0.1193,  ...,  0.0912, -0.8911,  0.0967]])\n",
      "average training loss for model1 : 2.2230353355407715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = batches[1]\n",
    "train_model_one_step(muP, x, y, normalize_first=True)\n",
    "muP_2 = deepcopy(muP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "muP.eval()\n",
    "muP_0.eval()\n",
    "muP_1.eval()\n",
    "muP_2.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_scales = muP.layer_scales\n",
    "intermediate_layer_keys = [\"layer_{:,}_intermediate\".format(l) for l in range(2, L + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define W0 and b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = {1: layer_scales[0] * muP_0.input_layer.weight.data.detach() / math.sqrt(muP_0.d + 1)}\n",
    "for i, l in enumerate(range(2, L + 1)):\n",
    "    layer = getattr(muP_0.intermediate_layers, intermediate_layer_keys[i])\n",
    "    W0[l] = layer_scales[l-1] * layer.weight.data.detach()\n",
    "\n",
    "W0[L+1] = layer_scales[L] * muP_0.output_layer.weight.data.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = layer_scales[0] * muP_0.input_layer.bias.data.detach() / math.sqrt(muP_0.d + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Delta_W_1 and Delta_b_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Delta_W_1 = {1: layer_scales[0] * (muP_1.input_layer.weight.data.detach() -\n",
    "                                       muP_0.input_layer.weight.data.detach()) / math.sqrt(muP_1.d + 1)}\n",
    "    for i, l in enumerate(range(2, L + 1)):\n",
    "        layer_1 = getattr(muP_1.intermediate_layers, intermediate_layer_keys[i])\n",
    "        layer_0 = getattr(muP_0.intermediate_layers, intermediate_layer_keys[i])\n",
    "        Delta_W_1[l] = layer_scales[l-1] * (layer_1.weight.data.detach() -\n",
    "                                            layer_0.weight.data.detach())\n",
    "\n",
    "    Delta_W_1[L+1] = layer_scales[L] * (muP_1.output_layer.weight.data.detach() -\n",
    "                                        muP_0.output_layer.weight.data.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Delta_b_1 = layer_scales[0] * (muP_1.input_layer.bias.data.detach() -\n",
    "                                   muP_0.input_layer.bias.data.detach()) / math.sqrt(muP_1.d + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Delta_W_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Delta_W_2 = {1: layer_scales[0] * (muP_2.input_layer.weight.data.detach() -\n",
    "                                       muP_1.input_layer.weight.data.detach()) / math.sqrt(muP_2.d + 1)}\n",
    "    for i, l in enumerate(range(2, L + 1)):\n",
    "        layer_2 = getattr(muP_2.intermediate_layers, intermediate_layer_keys[i])\n",
    "        layer_1 = getattr(muP_1.intermediate_layers, intermediate_layer_keys[i])\n",
    "        Delta_W_2[l] = layer_scales[l-1] * (layer_2.weight.data.detach() -\n",
    "                                            layer_1.weight.data.detach())\n",
    "\n",
    "    Delta_W_2[L+1] = layer_scales[L] * (muP_2.output_layer.weight.data.detach() -\n",
    "                                        muP_1.output_layer.weight.data.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Delta_b_2 = layer_scales[0] * (muP_2.input_layer.bias.data.detach() -\n",
    "                                   muP_1.input_layer.bias.data.detach()) / math.sqrt(muP_1.d + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore at step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On examples from the second batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = batches[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x2 = {0: x}\n",
    "    h0 = {1: F.linear(x, W0[1], b0)}\n",
    "    delta_h_1 = {1: F.linear(x, Delta_W_1[1], Delta_b_1)}\n",
    "    delta_h_2 = {1: F.linear(x, Delta_W_2[1], Delta_b_2)}\n",
    "    h1 = {1: layer_scales[0] * muP_1.input_layer.forward(x) / math.sqrt(muP_1.d + 1)}\n",
    "    h2 = {1: layer_scales[0] * muP_2.input_layer.forward(x) / math.sqrt(muP_2.d + 1)}\n",
    "    x2[1] = muP_2.activation(h2[1])\n",
    "    \n",
    "    torch.testing.assert_allclose(h0[1] + delta_h_1[1], h1[1], rtol=1e-5, atol=1e-5)\n",
    "    torch.testing.assert_allclose(h0[1] + delta_h_1[1] + delta_h_2[1], h2[1], rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_1 = delta_h_1[1] * delta_h_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5487)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prod_1 < 0).sum() / prod_1.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4941)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_1[1] < 0).sum() / delta_h_1[1].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3548)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[1] < 0).sum() / delta_h_2[1].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9147)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h0[1][0, :].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2890)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1[1][0, :].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7712)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2[1][0, :].abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, l in enumerate(range(2, L + 1)):\n",
    "        layer_1 = getattr(muP_1.intermediate_layers, intermediate_layer_keys[i])\n",
    "        layer_2 = getattr(muP_2.intermediate_layers, intermediate_layer_keys[i])\n",
    "        x = x2[l-1]\n",
    "\n",
    "        h0[l] =  F.linear(x, W0[l])\n",
    "        delta_h_1[l] = F.linear(x, Delta_W_1[l])\n",
    "        delta_h_2[l] = F.linear(x, Delta_W_2[l])\n",
    "        \n",
    "        h1[l] = layer_scales[l-1] * layer_1.forward(x)\n",
    "        h2[l] = layer_scales[l-1] * layer_2.forward(x)\n",
    "        x2[l] = muP_2.activation(h2[l])\n",
    "        \n",
    "        torch.testing.assert_allclose(h0[l] + delta_h_1[l], h1[l], rtol=1e-5, atol=1e-5)\n",
    "        torch.testing.assert_allclose(h0[l] + delta_h_1[l] + delta_h_2[l], h2[l], rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = x2[L] \n",
    "    h0[L+1] = F.linear(x, W0[L+1])\n",
    "    delta_h_1[L+1] = F.linear(x, Delta_W_1[L+1])\n",
    "    delta_h_2[L+1] = F.linear(x, Delta_W_2[L+1])\n",
    "    h1[L+1] = layer_scales[L] * muP_1.output_layer.forward(x)\n",
    "    h2[L+1] = layer_scales[L] * muP_2.output_layer.forward(x)\n",
    "    x2[L+1] = muP_2.activation(h2[L+1])\n",
    "                              \n",
    "    torch.testing.assert_allclose(h0[L+1] + delta_h_1[L+1], h1[L+1], rtol=1e-5, atol=1e-5)\n",
    "    torch.testing.assert_allclose(h0[L+1] + delta_h_1[L+1] + delta_h_2[L+1], h2[L+1], rtol=1e-5, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_1 = delta_h_1[2] * delta_h_2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5046)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(prod_1 < 0).sum() / prod_1.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3548)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[1] < 0).sum() / delta_h_2[1].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3726)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[2] < 0).sum() / delta_h_2[2].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3658)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[3] < 0).sum() / delta_h_2[3].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3825)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[4] < 0).sum() / delta_h_2[4].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3916)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[5] < 0).sum() / delta_h_2[4].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3795)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[6] < 0).sum() / delta_h_2[6].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4650)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(delta_h_2[7] < 0).sum() / delta_h_2[7].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.1566e-03,  1.8271e-03,  6.0032e-04,  ...,  1.0044e-03,\n",
       "          6.4865e-03,  1.3175e-03],\n",
       "        [ 2.2221e-02, -3.1599e-03,  2.5931e-03,  ..., -2.8529e-03,\n",
       "          8.0231e-03,  9.9353e-05],\n",
       "        [ 1.1129e-02,  1.3531e-04, -1.2471e-03,  ...,  3.8138e-04,\n",
       "          6.5811e-03,  1.1236e-03],\n",
       "        ...,\n",
       "        [ 1.6294e-02, -1.8284e-03, -6.6554e-04,  ..., -6.8285e-04,\n",
       "          6.1427e-03,  3.1220e-04],\n",
       "        [ 6.5086e-03,  7.7496e-03,  2.3772e-03,  ...,  3.6495e-04,\n",
       "          8.8420e-03, -3.3902e-04],\n",
       "        [ 1.8702e-02, -2.1560e-03,  1.8816e-03,  ...,  1.1074e-04,\n",
       "          9.8820e-03,  9.3896e-04]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_h_2[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 6, 3, 2, 7, 8, 2, 1, 6, 1, 2, 2, 3, 0, 2, 3, 0, 1, 2, 2, 2, 2, 7, 2,\n",
       "        0, 6, 2, 6, 0, 2, 2, 8, 1, 2, 2, 0, 2, 2, 2, 3, 0, 2, 7, 2, 7, 2, 0, 0,\n",
       "        8, 2, 8, 2, 8, 0, 7, 2, 4, 0, 7, 1, 3, 3, 7, 1, 6, 7, 2, 0, 1, 7, 2, 4,\n",
       "        8, 2, 0, 1, 2, 0, 8, 2, 0, 8, 3, 3, 2, 2, 1, 1, 2, 7, 2, 7, 1, 0, 8, 8,\n",
       "        2, 2, 2, 8, 1, 8, 0, 2, 7, 8, 8, 1, 1, 8, 1, 2, 7, 0, 7, 1, 1, 3, 0, 8,\n",
       "        6, 3, 8, 2, 7, 2, 2, 8, 8, 0, 1, 3, 0, 2, 0, 2, 2, 1, 8, 2, 0, 2, 2, 2,\n",
       "        2, 2, 6, 0, 3, 2, 1, 0, 0, 2, 2, 9, 2, 4, 0, 0, 9, 7, 2, 0, 0, 2, 2, 9,\n",
       "        2, 2, 7, 1, 9, 2, 7, 3, 9, 3, 6, 2, 0, 1, 2, 2, 7, 8, 3, 2, 2, 2, 0, 3,\n",
       "        0, 8, 0, 2, 8, 2, 1, 3, 8, 0, 2, 2, 4, 7, 0, 7, 2, 6, 8, 7, 0, 7, 7, 2,\n",
       "        2, 2, 7, 7, 8, 3, 2, 0, 2, 0, 2, 7, 0, 1, 6, 1, 1, 8, 8, 8, 2, 2, 8, 2,\n",
       "        8, 2, 2, 2, 2, 2, 8, 9, 2, 0, 2, 8, 8, 2, 2, 6, 7, 0, 8, 3, 3, 8, 2, 1,\n",
       "        2, 2, 0, 2, 7, 2, 2, 3, 0, 8, 0, 8, 8, 2, 0, 2, 0, 9, 3, 2, 2, 8, 8, 7,\n",
       "        8, 7, 8, 2, 2, 0, 1, 2, 2, 0, 8, 7, 8, 1, 2, 3, 2, 2, 2, 2, 7, 7, 1, 2,\n",
       "        6, 0, 1, 6, 0, 2, 6, 0, 2, 2, 0, 0, 9, 0, 2, 0, 9, 0, 2, 8, 0, 2, 8, 0,\n",
       "        2, 0, 7, 1, 3, 2, 3, 2, 8, 2, 8, 7, 2, 9, 6, 2, 6, 0, 2, 7, 7, 2, 2, 6,\n",
       "        2, 8, 1, 8, 2, 2, 7, 0, 7, 1, 7, 2, 2, 0, 7, 6, 2, 2, 7, 2, 1, 6, 6, 3,\n",
       "        2, 2, 3, 8, 2, 0, 1, 1, 2, 7, 0, 2, 1, 8, 0, 1, 7, 6, 2, 1, 7, 0, 1, 0,\n",
       "        2, 7, 2, 6, 2, 7, 0, 2, 3, 2, 1, 3, 9, 7, 7, 3, 2, 0, 9, 2, 1, 1, 2, 8,\n",
       "        2, 0, 6, 2, 2, 1, 7, 2, 9, 8, 2, 2, 0, 6, 2, 2, 7, 0, 2, 2, 2, 7, 2, 0,\n",
       "        3, 2, 2, 2, 7, 2, 0, 7, 0, 0, 7, 8, 2, 9, 3, 8, 8, 2, 2, 2, 2, 9, 2, 2,\n",
       "        2, 6, 8, 1, 2, 2, 0, 6, 8, 0, 1, 7, 2, 0, 2, 1, 2, 2, 7, 2, 3, 1, 2, 9,\n",
       "        1, 2, 0, 2, 8, 0, 1, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h0[7][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_1[7][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 3, 0, 0, 0, 0, 3, 0, 3, 0, 0, 3, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 3, 0, 0, 0, 3, 3, 3, 0, 0, 0, 0, 3, 3, 3, 0, 0, 0, 3, 3, 3, 0, 0,\n",
       "        3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 0, 3, 0, 3, 3, 0, 3, 3, 3, 0,\n",
       "        0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 3, 3, 0, 3, 3, 3, 0, 0, 3, 0, 3, 0, 0, 0,\n",
       "        0, 3, 0, 3, 3, 3, 0, 0, 0, 3, 0, 3, 3, 3, 3, 0, 3, 0, 0, 3, 3, 3, 0, 3,\n",
       "        0, 3, 3, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 3, 0, 3, 3, 3, 3, 0, 0, 0, 3, 0,\n",
       "        3, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 3,\n",
       "        0, 3, 0, 3, 0, 3, 3, 3, 0, 3, 0, 0, 0, 3, 0, 3, 3, 0, 3, 0, 0, 0, 0, 3,\n",
       "        0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0,\n",
       "        0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 0, 3, 0, 0,\n",
       "        3, 0, 3, 0, 0, 0, 0, 3, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 3,\n",
       "        0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0,\n",
       "        3, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 3, 0, 3, 0, 0, 0, 3, 0,\n",
       "        0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0,\n",
       "        0, 0, 0, 3, 3, 0, 3, 0, 3, 0, 0, 3, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0,\n",
       "        0, 3, 3, 0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3,\n",
       "        0, 3, 3, 3, 0, 0, 3, 3, 0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 3, 3, 0, 0, 3, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0,\n",
       "        3, 0, 0, 0, 3, 3, 3, 3, 0, 3, 3, 0, 0, 0, 3, 3, 3, 0, 0, 0, 3, 0, 0, 0,\n",
       "        3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 3, 0, 3, 0, 0,\n",
       "        0, 0, 0, 3, 3, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 3, 0, 3, 3, 0, 3, 3, 0, 0,\n",
       "        3, 0, 0, 0, 0, 0, 3, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_2[7][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 6, 3, 2, 7, 8, 2, 1, 6, 1, 2, 2, 3, 0, 2, 3, 0, 1, 2, 2, 2, 2, 7, 2,\n",
       "        0, 6, 2, 6, 0, 2, 2, 8, 1, 2, 2, 0, 2, 2, 2, 3, 0, 2, 7, 2, 7, 2, 0, 0,\n",
       "        8, 2, 8, 2, 8, 0, 7, 2, 4, 0, 7, 1, 3, 3, 7, 1, 6, 7, 2, 0, 1, 7, 2, 4,\n",
       "        8, 2, 0, 1, 8, 0, 8, 2, 0, 8, 3, 3, 2, 2, 1, 1, 2, 7, 8, 7, 1, 0, 8, 8,\n",
       "        2, 2, 2, 8, 1, 8, 0, 2, 7, 8, 8, 1, 1, 8, 1, 2, 7, 0, 7, 1, 1, 3, 0, 8,\n",
       "        6, 3, 8, 2, 7, 2, 2, 8, 8, 0, 1, 3, 0, 8, 0, 2, 2, 1, 8, 2, 0, 2, 2, 2,\n",
       "        2, 2, 6, 0, 3, 2, 1, 0, 0, 2, 2, 9, 2, 4, 0, 0, 9, 7, 2, 0, 0, 8, 2, 9,\n",
       "        2, 2, 7, 1, 9, 2, 7, 3, 9, 3, 6, 2, 0, 1, 2, 2, 7, 8, 3, 2, 2, 2, 0, 3,\n",
       "        0, 8, 0, 2, 8, 2, 1, 3, 8, 0, 2, 2, 4, 7, 0, 7, 2, 6, 8, 7, 0, 7, 7, 2,\n",
       "        2, 2, 7, 9, 8, 3, 2, 0, 2, 0, 2, 7, 0, 1, 6, 1, 1, 8, 8, 8, 2, 2, 8, 2,\n",
       "        8, 2, 8, 2, 2, 2, 8, 9, 2, 0, 2, 8, 8, 2, 2, 2, 7, 0, 8, 3, 3, 8, 2, 1,\n",
       "        2, 2, 0, 2, 7, 2, 2, 3, 0, 8, 0, 8, 8, 2, 0, 2, 0, 9, 3, 2, 2, 8, 8, 7,\n",
       "        8, 7, 8, 0, 2, 0, 1, 2, 9, 0, 8, 7, 8, 1, 2, 3, 2, 8, 2, 2, 7, 7, 1, 2,\n",
       "        6, 0, 1, 6, 0, 2, 6, 0, 2, 2, 0, 0, 9, 0, 2, 0, 9, 0, 2, 8, 0, 2, 8, 0,\n",
       "        2, 0, 7, 1, 3, 2, 3, 2, 8, 2, 8, 7, 2, 9, 6, 2, 6, 0, 2, 7, 7, 2, 2, 0,\n",
       "        2, 8, 1, 8, 2, 2, 7, 0, 7, 1, 7, 2, 2, 0, 7, 6, 2, 2, 7, 2, 1, 6, 6, 3,\n",
       "        2, 2, 3, 8, 2, 0, 1, 1, 2, 7, 0, 2, 1, 8, 0, 1, 7, 2, 2, 1, 7, 0, 1, 0,\n",
       "        2, 7, 2, 6, 2, 7, 0, 1, 3, 2, 1, 3, 9, 7, 7, 3, 2, 0, 9, 2, 1, 1, 2, 8,\n",
       "        2, 0, 3, 2, 2, 1, 7, 2, 9, 8, 1, 2, 0, 6, 2, 2, 7, 0, 2, 2, 2, 7, 8, 0,\n",
       "        3, 2, 2, 2, 7, 2, 0, 7, 0, 0, 7, 8, 2, 9, 3, 8, 8, 2, 2, 2, 2, 9, 2, 2,\n",
       "        2, 6, 8, 1, 2, 2, 0, 2, 8, 0, 1, 7, 2, 0, 2, 1, 2, 2, 7, 2, 3, 1, 2, 9,\n",
       "        1, 2, 0, 2, 8, 0, 1, 8])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h2[7][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 441,  494,  958,   49,  223,  813,   92,  910,  456,  416,   12,   28,\n",
       "         867,  603,  612,  291,  568,  541,   86, 1015,  630,  405,  834,  910,\n",
       "         745,  291,  713,  166,  561,  543,  494,   86,  416,  828,  630,  568,\n",
       "         494,  199,  910,  675,  117,  206,  855,  703,  910,  422,  187,  790,\n",
       "          53,  910,  427,  206,  943,   86,  755,  416,  311,  568,  703,  723,\n",
       "         731,  760,  661,  227,  836, 1015,  206,   11,  227,   42,  206,  813,\n",
       "         910,  942,  745,  227,  510,  403,   86,    9,  117,  291,  209, 1018,\n",
       "          86,  910,  227,  589,   19,  703,  144,  509,  227,  612,   86,  903,\n",
       "         165,  206,   12,  958,  227,  706,  117, 1015,  703,  498,  958,  910,\n",
       "         910,  790,  910,  154,  910,   86,  945,  713,  227,  706,  612,  731,\n",
       "         813,  469,  346,  510,  985,  696,  782,  133,  755,  439,  910,  760,\n",
       "          23,  910,  971,   86,  427,  723,  713,  612,  372,  976,  498,  416,\n",
       "         910,  257,   49,  354,  235,  612,  752,  439,  568,  789,  813,  976,\n",
       "         307,  570,  187,  230,   92,  755, 1015,  439,   11,   51,  524,  976,\n",
       "         514,  756,  976,  723,  439,  892,  892,  958,  199,  760,  441,   90,\n",
       "         439,  683,   92,   86,  910,  612,  942,  416,  223,  150,  330,  350,\n",
       "         117,   59,  684,  818,  976,  142,  227,  199,  346,  117,  427,  870,\n",
       "         427,  176,  612,  755,  882,  894,  942,  985,  570,  910,  755,  654,\n",
       "         319,  206,  755,  813,   86,  708,   86,   92,  570,  117,  416,  755,\n",
       "         790,   74,  206,  416,  910,  813,  291,  760,  416,  494,  452,  813,\n",
       "         699,  230,  752,  855,  745,  954,  206,  766,  494,  708,  630,   86,\n",
       "         975,   86,   12,   92,  828,  494,  942,  708,  790,  199,  612,  416,\n",
       "         703,  291,  817,  612,  703,  368,  187,  616,   64,   86,  330,  612,\n",
       "         478,  154,  971,  531,  696,  958,  346,  745,  870,  291,  942,  752,\n",
       "          42,  185,  153,  144,  855,  743,  227,  416,  274,  766,   86,  869,\n",
       "         958,  227,  795,  836,  154,   86,  199,  660,  176,  703,  227,  752,\n",
       "         291,  615,  227,  957,  385,  789,  291,  439,   92,  976,  223,  568,\n",
       "         854,  469,   86,  903,  945,  612,  206,  960,  760,  550,   86,  439,\n",
       "         291,  612,  910,  227,  731,  789,  912,  758,  369,  954,  144,  755,\n",
       "         612,  713,  987,  910,   12,  556,    9,  910,  869, 1015,  206,  612,\n",
       "         612,  703,  723,  346,  368,  340,   31,  330,  910,  227,  447,  612,\n",
       "         494,  354,   92,  890, 1015,  726,  248,  612,  334,  403,  860,  346,\n",
       "         340,  206,  510,  790,  206,  439,  910,   74,  987,  910,  696,  416,\n",
       "         723,  206,  287,  681,  833,  291,  243,   74,  187,   11,  227,  367,\n",
       "         531,  910,  154,  570,  813,  910,  230,  660,   84,  612,  120,  209,\n",
       "         105,  703,  910,  790,  319,  760,  945,  140,  910,  311,  554,  636,\n",
       "         510,  291,  291,  471,  133,  425,  176,  723,  854,  206,  971,  510,\n",
       "           8,  199,  570,  422,  311,  790,  336,   66,  494,  954,  439,  306,\n",
       "         144,  291,  646,  510,   42,  510,  497,  910,  330,  760,   42,  291,\n",
       "          86,  945,  291,  942,  427,  427,   92,   49,  752,  854,  942,  760,\n",
       "         815,  612,   86,   74,  343,  412,   11,  368,  942,  439,  334,  910,\n",
       "         154,   11,   86,  311,  416,  723,  542,  871,  971,  227,  319,  172,\n",
       "         227,   86,  875,  813,  760,  291,  425,  206])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h0[1][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 541,  524,  602,  541,  541,  541,  356,   68,  524,   68,  524,  524,\n",
       "         602,  541,   25,  602,  790,   68,  524,  524,  541,  541,  541,  541,\n",
       "         156,  524,   68,  524,  541,  541,   68,   68,   68,  156,  524,  790,\n",
       "         524,  541,  541,  602,  839,  853,  541,  541,  541,  120,  524,  541,\n",
       "          68,   68,  541,  156,  541,  524,  541,  524,  524,  524,  541,   68,\n",
       "         602,  602,  356,   68,  524,  356,   68,  524,   68,  541,  199,  524,\n",
       "         541,  524,  839,   68,  602,  790,  541,  524,  790,  541,  602,  602,\n",
       "         541,  541,   68,   68,  541,  541,   68,  541,   68,  541,  541,  541,\n",
       "         524,   68,  524,  541,   68,  602,  790,  524,  541,  541,  541,   68,\n",
       "          68,  541,   68,  541,  541,  524,  541,   68,   68,  602,  524,  541,\n",
       "         524,  602,  602,  524,  541,  524,  302,  541,  541,  156,   68,  602,\n",
       "         541,   68,  602,   68,  337,   68,  541,  524,  528,  541,   68,  156,\n",
       "          68,  261,  524,  790,   68,  524,   68,  790,  602,  156,  541,  541,\n",
       "          68,  524,  790,  524,  541,  541,  524,  790,  524,  541,  524,  541,\n",
       "         524,  611,  541,   68,  541,  541,  541,  602,  541,  602,  524,  524,\n",
       "         790,   68,  302,  541,  541,  541,  541,  156,  541,  541,  790,  602,\n",
       "         156,  541,  541,  524,  541,  524,   68,  602,  541,  839,  611,  541,\n",
       "         524,  541,  524,  541,  524,  524,  541,  541,  602,  541,  541,  611,\n",
       "         853,   68,  541,  541,  541,  602,  228,  541,  356,  790,  524,  541,\n",
       "         524,   68,  524,   68,   68,  541,  524,  120,  541,  524,  228,  524,\n",
       "         228,  302,  541,  524,  935,  524,  541,  302,  136,  790,  541,  541,\n",
       "         524,  356,  524,  356,  541,  524,  541,  602,  602,  524,  524,   68,\n",
       "         541,  524,  524,   68,  541,  524,  528,  602,  541,  541,  790,  541,\n",
       "         541,  541,  602,   25,  790,  524,  602,  524,  524,  602,  541,  541,\n",
       "         541,  541,  541,  524,  524,  323,   68,  524,  541,  524,  228,  667,\n",
       "         541,   68,  524,  602,  541,  541,  541,  524,  541,  541,   68,  541,\n",
       "         524,  524,   68,  524,  302,  302,  524,  790,  524,  524,  524,  790,\n",
       "         541,  524,  602,  524,  541,  524,   68,  602,  602,  524,  541,  790,\n",
       "         602,  524,  541,   68,  602,  337,  602,  302,  602,  524,  541,  541,\n",
       "         541,  541,  524,  120,  524,  602,   68,  541,  667,  302,  541,  602,\n",
       "         528,  541,   68,  602,  524,  356,  541,  524,  541,   68,  541,  337,\n",
       "         156,  524,  541,  524,  302,  541,  541,  524,   68,  524,  524,  602,\n",
       "         541,   68,  602,  199,  853,  790,   68,   68,  524,  541,  541,  528,\n",
       "          68,  541,  524,   68,  350,  524,  541,   68,  541,  524,   68,  524,\n",
       "         261,  541,  541,  524,  356,  541,  541,  611,  602,  524,   68,  602,\n",
       "         541,  541,  541,  602,  156,  790,  541,  337,   68,   68,  528,  602,\n",
       "         524,  524,  602,  524, 1006,   68,  541,   68,  541,  120,   68,  709,\n",
       "         790,  524,  541,  524,  541,  541,  524,  337,   68,  356,  602,  156,\n",
       "         602,  524,   68,  541,  541,  524,  156,  541,  790,  790,  541,  541,\n",
       "         524,  541,  602,  120,  228,  199,  524,   68,  524,  302,  541,  528,\n",
       "         853,  524,  541,   68,  541,  524,  524,  524,  120,  156,   68,  541,\n",
       "         524,  602,  524,   68,  524,   68,  541,  524,  602,   68,  524,  541,\n",
       "          68,  524,  602,  541,  541,  602,   68,  541])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_1[1][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 908, 1022,  728,  908,  908,  908,  908,  824, 1022,  824, 1022, 1022,\n",
       "         728,  167,  824,  728,  760,  824, 1022, 1022, 1022, 1022,  167,  908,\n",
       "         760, 1022,  824, 1022,  187,  908,  824,  187,  824, 1022,  908,   18,\n",
       "        1022,  187,  908,  728,  760, 1022,  908,  908,  908, 1022,  187,  187,\n",
       "         187,  824, 1022, 1022,  167,  187,  167, 1022,  908, 1022,  908,  588,\n",
       "         728,  728,  908,  588,  728,  908, 1022, 1022,  588,  908,  728,  908,\n",
       "         728, 1022,  760,  824,  187,  760,  187, 1022,  230, 1022,  728,  728,\n",
       "         908,  908,  824,  824,  908,  167,  187,  908,  824,  728,  908,  187,\n",
       "        1022,  824, 1022,  187,  824,  187,  760,  824,  908, 1022,  908,  824,\n",
       "         824,  187,  824,  908,  908,  230,  908,  824,  824,  728,  187,  187,\n",
       "        1022,  187,  728, 1022,  167, 1022,  908, 1022,  187,   18,  824,  728,\n",
       "        1022, 1022,  760,  187, 1022,  824, 1022, 1022,  760, 1022,  824,  824,\n",
       "         824,  824, 1022,   18,  728, 1022,  824,  760,  728, 1022,  908,  908,\n",
       "        1022,  908,  728,  230,  187,  908, 1022,  760, 1022,  187,  187,  908,\n",
       "        1022,  824,  908,  824,  908,  908,  908,  728,  908,  728, 1022, 1022,\n",
       "          18,  824,  908,  908,  908, 1022,  187,  824, 1022,  908,  230,  728,\n",
       "         760,  908,  167, 1022, 1022, 1022,  824,  728,  187,  230,  824,  908,\n",
       "         908,  908,  187,  167, 1022, 1022,  187,  908,  728,  908,   57, 1022,\n",
       "         824,  824,  908,  908,  728,  728, 1022, 1022, 1022,  760, 1022,  167,\n",
       "         230,  824, 1022,  824, 1022, 1022, 1022,  728,  908,  824, 1022, 1022,\n",
       "         728, 1022, 1022,  187,  211,  908, 1022,  908, 1022,  728, 1022,  187,\n",
       "         728, 1022, 1022, 1022,  908,  188,  187,  728,  728, 1022, 1022,  824,\n",
       "        1022, 1022, 1022,  824,  908, 1022, 1022,  728, 1022,  187,  230, 1022,\n",
       "         187, 1022,  728, 1022,  230,  908,  728, 1022, 1022,  728,  187,  908,\n",
       "         187,  908, 1022, 1022, 1022,  760,  824, 1022,  908,  187, 1022,  908,\n",
       "         908,  824, 1022,  728,  908,  187,  187, 1022,  908,  908,  824,  908,\n",
       "        1022, 1022,  824, 1022, 1022,  211, 1022,   18,  908,  908, 1022,  728,\n",
       "         908,  187,  187,  728,  908,  187, 1022,  728,  760, 1022,  187,  760,\n",
       "         187,  187,  908,  824,  728, 1022,  187, 1022,  728, 1022, 1022,  167,\n",
       "        1022,  908, 1022, 1022, 1022,  760,  824,  908,  167, 1022, 1022,  728,\n",
       "         187, 1022,  824,  728, 1022,  908,  908,  728,   57,  824,  908,  824,\n",
       "         187,  230,  908,  187, 1022,  908,  167, 1022,  824, 1022, 1022,  728,\n",
       "         908,  824,  187,  728, 1022,   18,  824,  187, 1022,  167,  187,  824,\n",
       "         824,  187,  187,  824,  908, 1022, 1022,  824,  167, 1022,  824, 1022,\n",
       "        1022,  908, 1022, 1022,  908,  908,  187,  824,  187, 1022,  824,  187,\n",
       "         908,  908,  908,  728,  728,  728,  908, 1022,  824,  588, 1022,  728,\n",
       "         824,  728,  728, 1022, 1022,  824,  908,  824,  908, 1022,  187,  728,\n",
       "          18, 1022,  908, 1022,  167,  187, 1022,  728,  824,  908,  728,  760,\n",
       "         728, 1022,  529,  187,  908, 1022,  760,  908,  230,  187,  908,  187,\n",
       "        1022,  908,  728, 1022, 1022,  728, 1022,  908, 1022,  908, 1022, 1022,\n",
       "        1022, 1022, 1022,  824,  908, 1022,  187, 1022,  187,  760,  824,  908,\n",
       "        1022,  728, 1022,  824, 1022,  824,  908,  187,  187,  824, 1022,  908,\n",
       "         588, 1022,  657,  908,  187,  728,  824, 1022])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_2[1][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 228, 1022,  187,  602,  187,  908, 1022,  228,  602,  681, 1022, 1022,\n",
       "         728,  602, 1022,  187,  230,  801, 1022, 1022, 1022, 1022,  187,  657,\n",
       "         230,  602,  801,  656,  230,  908,  494,   57,  801, 1022,  293,  230,\n",
       "         494,  657,  228,  728,  760, 1022,   57,  154,  228,  494,  187,  790,\n",
       "          57,  494,  908,  206,  908,  230,  187, 1022,  602,  230,   57,  681,\n",
       "          57,  760,  293,  588,  188,  494,  206,  230,  894,   57,  206,  908,\n",
       "         657,  765,  230,  228,  602,  230,   57, 1022,  230,  790,  507,  728,\n",
       "         908,  908,  588,  588,  228,  187,   17,  177,  681,  790,  228,  507,\n",
       "        1022,  878, 1022,  602,  681,  602,  230,  211,  167,  657,  657,  824,\n",
       "          57,  808,  681,  908,  228,  230,  167,  801,  588,  602,  230,  602,\n",
       "        1022,   57,  602,  228,  167, 1022, 1022, 1022,  602,  230,  588,  760,\n",
       "        1022,  790,  187,   57,  529,   57,  228, 1022,  230, 1022,  494, 1022,\n",
       "          57,  494,  524,  230,  187, 1022,   57,  230,  230, 1022,  885,  908,\n",
       "         228,  657,  230,  230,  602,  167, 1022,  230,  230,  228,   57,  908,\n",
       "        1022,  206,  908,   57,  908,  602,  908,  187,  908,  602, 1022,  908,\n",
       "         230,  588,  602,  808,   57,  228,   57, 1022, 1022,  908,  230,   57,\n",
       "         230,  228,  187, 1022,  657, 1022,  681,  602,  602,  230,  206,  908,\n",
       "         908,   57, 1022,  167, 1022,   17,  790,  908,  728,  167,   57,  423,\n",
       "         494,  494,  167,  908,  790,  728, 1022, 1022,  908,  230, 1022,  167,\n",
       "         230,  801, 1022,   57,  228,  657,  790,  790,  602,  494,  228,  657,\n",
       "         790, 1022,  228, 1022,  287, 1022,  790,  602,  494,  187,  228,  908,\n",
       "         790,  228, 1022,  602,  350,  188,  602,  187,  790,  529, 1022,  494,\n",
       "         206, 1022,  908,  824,  167,   17, 1022,   57,  230,  790,  230,  228,\n",
       "         760, 1022,  728, 1022,  230,  602,   57, 1022, 1022,   17,  728,   38,\n",
       "         187,   57,  790, 1022, 1022,  230,  588, 1022,  602,  187, 1022,  187,\n",
       "         602,  228, 1022,  728,  602,  790,  885,  529,  167,  167,  681,  908,\n",
       "         602, 1022,  681, 1022,    2, 1022, 1022,  230, 1022,  908,  230,  230,\n",
       "         602,  230,  187,  728,  187,   17,  824,   17,  230, 1022,  602,  230,\n",
       "         602,  230,  187,  894,  187,  529,  602, 1022,  228,  293,  228,  167,\n",
       "        1022,  908,  885, 1022, 1022,  760,  136,  908,  187,  602,  228,   17,\n",
       "        1022,  228,   57,  602, 1022,  154,   57,  230,   57,  824,  908,  824,\n",
       "          17,  230,  167,  885, 1022, 1022,  167, 1022,  681,  885, 1022,   57,\n",
       "         602,  206,   57,  790,  206,  230,  824,  228,  494,  167,  230, 1022,\n",
       "          57,  228,  230,  228,  293, 1022,  602,  681,  187,  230,  588,  790,\n",
       "         187,  167, 1022,  602,  908,  187,  230,  206,  602, 1022,  681,  187,\n",
       "         602,  657,  167,  790,   17,  230,  908,  529,  801,  681, 1022,  526,\n",
       "         602,   17,   17, 1022,  206,  801,  167,   57,  602,   57,   57,   17,\n",
       "         230,  657,  808,  494,  167,  230,  848, 1022,  494,  908,   17,  230,\n",
       "         187,  494,  529,   57,  187, 1022,  230,  167,  230,  760,  187,  657,\n",
       "        1022,  602,  602,  790,  228,  529, 1022,  154, 1022,  602,  885, 1022,\n",
       "        1022, 1022, 1022,  681,  908, 1022,  230, 1022,  728,  760,  681,  167,\n",
       "        1022,  187, 1022,  588, 1022,  494,   57,   17,  602,  588,  211,  602,\n",
       "         681, 1022,  187,  908,  602,  728,  425, 1022])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h2[1][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 156,  726,  726,  684,  769,  428,  684,  902,  271,  778,  726,  531,\n",
       "         726,  769,  726,  726,  726,  778,  531,  883,  156, 1021,  769,  684,\n",
       "         726,  428,  902, 1021,  726,  684,  156,  726,  778,  726,  883,  726,\n",
       "         726,  769,  902,  726,  726,  883,  769,  902,  769,  531,  726,  726,\n",
       "         902,  902,  769,  883,  531,  726,  769,  726,  684,  726,  246,  778,\n",
       "         221, 1021,  246,  902,  726,  246,  902,  531,  778,  769,  726,  428,\n",
       "         726,  531,  726,  902,  156,  726,  726,  726,  726,  726,  726,  726,\n",
       "         428,  661,  778,  246,  246,  769,  726,  769,  778,  726,  531,  726,\n",
       "         726,  726,  883,  769,  778,  726,  726,  156,  961,  726,  769,  902,\n",
       "         778,  769,  778,  684,  769,  726,  684,  778,  902,  726,  726,  726,\n",
       "         428,  221,  726,  156,  684,  726,  401,  726,  769,  726,  778,  726,\n",
       "         531,  902,  726,  902,  902,  902,  156,  726,  726,  726,  778,  726,\n",
       "         902, 1021,  531,  726,  726,  726,  902,  726,  221,  726,  684,  769,\n",
       "         902,  684,  726,  726,  684,  684,  726,  726,  787,  156,  531,  769,\n",
       "         684,  902,  769,  902,  769,  769,  769,  726,  769,  726,  726,  684,\n",
       "         726,  246,  684, 1021,  246,  726,  769,  726,  726,  769,  726,  221,\n",
       "         726,  769,  726,  531,  726,  531,  778,  221,  221,  726,  902,  428,\n",
       "         684,  661,  726,  769,  684,  726,  726,  769,  221,  661,  246,  883,\n",
       "         883,  902,  246,  769,  221,  726,  531,  769,  883,  726,  726,  769,\n",
       "         726,  902,  726,  902,  902,  726,  726,  726,  769, 1021,  531,  684,\n",
       "         156,  271,  531,  531,  850,  246,  531,  156,  883,  726,  902,  221,\n",
       "         726,  531,  726,  271, 1021,  726,  726,  726,  221,  221,  428,  778,\n",
       "         531,  726,  531,  883,  684,  726,  726,  221,  726,  726,  726,  156,\n",
       "         221,  684,  726,  531,  726,  684,  726,  726,  531,  726,  726,  661,\n",
       "         246,  769,  531,  531,  531,  726,  156,  726,  684,  726,  726,  684,\n",
       "         769,  778,  246,  726,  684,  726, 1021,  156,  246,  246,  902,  531,\n",
       "         428,  726,  778,  726,  138,  883,  428,  726,  684,  684,  726,  726,\n",
       "         684,  531,  726,  726,  246,  726,  156,  726,  726,  726,  769,  726,\n",
       "         726,  726,  684,  902,  726,  726, 1021,  684,  726,  428,  726,  246,\n",
       "         401,  769,  428,  531,  726,  726,  726,  769,  684,  684,  726,  726,\n",
       "         726,  531,  902,  726,  726,  902,  246,  726,  246,  778,  769,  726,\n",
       "         726,  726,  246,  726,  684,  684,  684,  726,  778,  726,  726,  726,\n",
       "         684,  902,  156,  726,  401,  726,  778,  246,  531,  684,  726,  726,\n",
       "         902,  156,  726,  778,  961,  726, 1021,  778,  769,  726,  778,  726,\n",
       "         787,  661,  684,  271,  684,  769,  726,  902,  950,  401,  778,  726,\n",
       "         769,  684,  661,  221,  726,  726,  246,  902,  902,  950,  883,  221,\n",
       "         640,  726,  726,  531,  902,  902,  661,  902,  769,  726,  902, 1021,\n",
       "         726,  726,  769,  902,  661,  726,  531,  726,  531,  684,  726,  726,\n",
       "         726,  726,  902,  902,  684, 1021,  726,  661,  726,  726,  769,  726,\n",
       "         531,  769,  221,  726,  531,  726,  726,  902,  401,  246,  883,  883,\n",
       "         726,  428,  726,  778,  769,  726,  726,  726,  726,  726,  778,  769,\n",
       "         428,  726,  726,  778,  726,  902,  769, 1021,  156,  778,  883,  726,\n",
       "         778,  531,  726,  684,  769,  726,  902,  726])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h0[2][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([797, 814, 895, 797, 895, 895, 895, 797, 814, 797, 814, 797, 340, 895,\n",
       "        814, 895, 340, 797, 895, 895, 797, 895, 895, 895, 340, 814, 797, 814,\n",
       "        340, 895, 797, 797, 797, 814, 797, 340, 814, 797, 797, 340, 340, 814,\n",
       "        895, 797, 797, 797, 340, 340, 797, 797, 797, 814, 797, 340, 895, 814,\n",
       "        797, 340, 797, 797, 797, 340, 797, 797, 814, 797, 797, 340, 797, 797,\n",
       "        797, 895, 895, 797, 340, 797, 797, 340, 895, 814, 340, 797, 340, 895,\n",
       "        895, 797, 797, 797, 797, 895, 797, 895, 797, 340, 895, 895, 814, 797,\n",
       "        814, 895, 797, 340, 340, 797, 797, 797, 895, 797, 797, 895, 797, 895,\n",
       "        797, 340, 895, 797, 797, 895, 340, 797, 895, 797, 814, 797, 895, 814,\n",
       "        895, 895, 895, 340, 797, 340, 895, 797, 340, 797, 797, 797, 797, 814,\n",
       "        340, 895, 797, 814, 797, 797, 797, 340, 814, 814, 797, 340, 340, 814,\n",
       "        895, 895, 797, 895, 340, 340, 895, 895, 814, 340, 340, 797, 797, 895,\n",
       "        895, 797, 895, 797, 895, 797, 895, 340, 895, 340, 814, 895, 340, 797,\n",
       "        895, 797, 797, 797, 797, 814, 895, 895, 340, 895, 340, 797, 340, 895,\n",
       "        895, 797, 797, 895, 895, 340, 797, 895, 895, 895, 340, 895, 895, 814,\n",
       "        895, 895, 895, 797, 797, 797, 814, 797, 797, 895, 895, 895, 814, 895,\n",
       "        895, 340, 814, 895, 340, 797, 814, 797, 797, 895, 797, 814, 895, 797,\n",
       "        797, 895, 797, 340, 797, 814, 814, 797, 797, 895, 814, 340, 797, 895,\n",
       "        895, 895, 814, 895, 797, 340, 895, 814, 895, 814, 814, 797, 797, 814,\n",
       "        895, 797, 895, 814, 814, 895, 340, 895, 340, 797, 895, 895, 340, 797,\n",
       "        340, 895, 797, 814, 814, 340, 895, 797, 895, 797, 797, 895, 814, 340,\n",
       "        797, 814, 895, 814, 814, 895, 895, 797, 797, 340, 895, 895, 797, 797,\n",
       "        895, 895, 797, 797, 814, 340, 797, 814, 340, 340, 814, 340, 895, 895,\n",
       "        340, 340, 895, 895, 340, 895, 895, 340, 797, 895, 340, 814, 895, 340,\n",
       "        814, 340, 895, 797, 895, 797, 895, 340, 797, 797, 797, 797, 797, 797,\n",
       "        814, 797, 814, 340, 797, 895, 895, 895, 797, 340, 814, 797, 797, 895,\n",
       "        814, 797, 797, 340, 797, 797, 895, 814, 814, 340, 797, 814, 814, 797,\n",
       "        340, 797, 797, 814, 814, 797, 895, 797, 797, 797, 814, 340, 797, 797,\n",
       "        814, 797, 340, 814, 797, 797, 814, 797, 797, 814, 797, 797, 340, 340,\n",
       "        797, 340, 340, 895, 895, 895, 895, 895, 340, 797, 797, 814, 797, 814,\n",
       "        895, 895, 895, 340, 814, 340, 797, 797, 797, 797, 814, 895, 797, 814,\n",
       "        814, 814, 797, 797, 895, 797, 895, 797, 797, 814, 340, 814, 797, 797,\n",
       "        797, 340, 797, 814, 797, 797, 797, 340, 895, 797, 797, 797, 895, 814,\n",
       "        340, 797, 340, 340, 895, 895, 895, 895, 797, 797, 797, 814, 895, 797,\n",
       "        814, 895, 797, 814, 814, 814, 895, 797, 895, 814, 340, 814, 797, 340,\n",
       "        797, 797, 895, 340, 814, 797, 814, 797, 895, 814, 797, 797, 814, 895,\n",
       "        797, 814, 340, 895, 895, 340, 797, 895])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_1[2][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([469, 469, 469, 469, 469, 469, 469, 975, 469, 975, 469, 469, 469, 469,\n",
       "        469, 469, 469, 975, 469, 469, 469, 469, 469, 469, 469, 469, 975, 469,\n",
       "        469, 469, 975, 469, 975, 469, 469, 469, 469, 469, 469, 469, 469, 490,\n",
       "        469, 469, 469, 469, 469, 469, 469, 975, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 975, 469, 469, 469, 975, 469, 469, 469, 469, 975, 469,\n",
       "        469, 469, 469, 469, 469, 975, 469, 469, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 975, 975, 469, 469, 469, 469, 975, 469, 469, 469, 469, 469,\n",
       "        469, 469, 975, 469, 469, 469, 469, 469, 469, 975, 975, 469, 975, 469,\n",
       "        469, 469, 469, 975, 975, 469, 469, 469, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 469, 975, 469, 469, 469, 469, 469, 469, 975, 469, 469,\n",
       "        469, 469, 975, 490, 469, 469, 469, 469, 469, 469, 975, 469, 469, 469,\n",
       "        469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469,\n",
       "        469, 401, 469, 975, 469, 469, 469, 469, 469, 469, 469, 469, 469, 975,\n",
       "        469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 975, 469, 469, 469, 975, 469, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 469, 469, 469, 469, 975, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 469, 469, 975, 469, 975, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 490, 469, 469, 469,\n",
       "        469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 975, 469, 469,\n",
       "        469, 975, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469,\n",
       "        975, 469, 469, 469, 469, 469, 469, 975, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 975, 469, 469, 469, 975, 469, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 975, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 469, 490, 469, 469, 469, 469, 469, 469, 469, 975, 469,\n",
       "        469, 469, 469, 469, 469, 975, 469, 490, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 975, 469, 469, 469, 469, 975, 469, 469, 490, 469, 975, 469,\n",
       "        469, 469, 469, 469, 975, 469, 469, 975, 469, 469, 469, 975, 469, 469,\n",
       "        975, 469, 469, 469, 469, 469, 469, 469, 469, 975, 469, 469, 975, 469,\n",
       "        469, 469, 469, 469, 469, 469, 469, 469, 975, 975, 490, 469, 469, 469,\n",
       "        469, 469, 469, 975, 469, 401, 469, 469, 975, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 975, 469, 469, 469,\n",
       "        469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469, 469,\n",
       "        469, 469, 469, 469, 490, 469, 469, 975, 469, 469, 469, 469, 469, 469,\n",
       "        975, 469, 469, 469, 469, 975, 469, 975, 469, 469, 469, 975, 469, 469,\n",
       "        975, 469, 469, 469, 469, 469, 975, 469])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(delta_h_2[2][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 156,  726,  726,  684,  769,  428,  684,  902,  271,  778,  726,  531,\n",
       "         726,  769,  726,  726,  726,  778,  531,  883,  156, 1021,  769,  684,\n",
       "         726,  428,  902, 1021,  726,  684,  156,  726,  778,  726,  883,  726,\n",
       "         726,  769,  902,  726,  726,  883,  769,  902,  769,  531,  726,  726,\n",
       "         902,  902,  769,  883,  531,  726,  769,  726,  684,  726,  246,  778,\n",
       "         221, 1021,  246,  902,  726,  246,  902,  531,  778,  769,  726,  428,\n",
       "         726,  531,  726,  902,  156,  726,  726,  726,  726,  726,  726,  726,\n",
       "         428,  661,  778,  246,  246,  769,  726,  769,  778,  726,  531,  726,\n",
       "         726,  726,  883,  769,  778,  726,  726,  156,  961,  726,  769,  902,\n",
       "         778,  769,  778,  684,  769,  726,  684,  778,  902,  726,  726,  726,\n",
       "         428,  221,  726,  156,  684,  726,  401,  726,  769,  726,  778,  726,\n",
       "         531,  902,  726,  902,  902,  902,  156,  726,  726,  726,  778,  726,\n",
       "         902, 1021,  531,  726,  726,  726,  902,  726,  221,  726,  684,  769,\n",
       "         902,  684,  726,  726,  684,  684,  726,  726,  787,  156,  531,  769,\n",
       "         684,  902,  769,  902,  769,  769,  769,  726,  769,  726,  726,  684,\n",
       "         726,  246,  684, 1021,  246,  726,  769,  726,  726,  769,  726,  221,\n",
       "         726,  769,  769,  531,  726,  531,  778,  221,  221,  726,  902,  428,\n",
       "         684,  661,  726,  769,  684,  726,  726,  769,  221,  661,  246,  883,\n",
       "         883,  902,  246,  769,  221,  726,  531,  769,  883,  726,  726,  769,\n",
       "         726,  902,  726,  902,  902,  726,  726,  726,  769, 1021,  531,  684,\n",
       "         156,  271,  531,  531,  850,  246,  531,  156,  883,  726,  902,  221,\n",
       "         726,  531,  726,  271, 1021,  726,  726,  726,  221,  221,  428,  778,\n",
       "         531,  726,  531,  883,  684,  726,  726,  221,  726,  726,  726,  401,\n",
       "         221,  684,  726,  531,  726,  684,  726,  726,  531,  726,  726,  661,\n",
       "         246,  769,  531,  531,  531,  726,  156,  726,  684,  726,  726,  684,\n",
       "         769,  778,  246,  726,  684,  726, 1021,  156,  246,  246,  902,  531,\n",
       "         428,  726,  778,  726,  138,  883,  428,  726,  684,  684,  726,  726,\n",
       "         684,  531,  726,  726,  246,  726,  156,  726,  726,  726,  769,  726,\n",
       "         726,  726,  684,  902,  726,  726, 1021,  684,  726,  428,  726,  246,\n",
       "         401,  769,  428,  531,  726,  726,  726,  769,  684,  684,  726,  726,\n",
       "         726,  531,  902,  726,  726,  902,  246,  726,  246,  778,  769,  726,\n",
       "         726,  726,  246,  726,  684,  684,  684,  726,  778,  726,  726,  726,\n",
       "         684,  902,  156,  726,  401,  726,  778,  246,  531,  684,  726,  726,\n",
       "         902,  156,  531,  778,  961,  726, 1021,  778,  769,  726,  778,  726,\n",
       "         787,  661,  684,  271,  684,  769,  726,  902,  950,  401,  778,  726,\n",
       "         769,  684,  661,  221,  726,  726,  246,  902,  902,  950,  883,  221,\n",
       "         640,  726,  726,  531,  902,  902,  661,  902,  769,  726,  902, 1021,\n",
       "         726,  726,  769,  902,  661,  726,  531,  726,  531,  684,  726,  726,\n",
       "         726,  726,  902,  902,  684, 1021,  726,  661,  726,  726,  769,  726,\n",
       "         531,  769,  221,  726,  531,  726,  726,  902,  401,  246,  883,  883,\n",
       "         726,  428,  726,  778,  769,  726,  726,  726,  726,  726,  778,  769,\n",
       "         428,  726,  726,  778,  726,  902,  769, 1021,  156,  778,  883,  726,\n",
       "         778,  531,  726,  684,  769,  726,  902,  726])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, b = torch.max(h2[2][:, :], dim=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9648)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, max_h2 = torch.max(h2[7], dim=1)\n",
    "_, max_h0 = torch.max(h0[7], dim=1)\n",
    "(max_h2 == max_h0).sum() / max_h2.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h0</th>\n",
       "      <th>delta_h_1</th>\n",
       "      <th>delta_h_2</th>\n",
       "      <th>h1</th>\n",
       "      <th>h2</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>69</td>\n",
       "      <td>42</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        h0 delta_h_1 delta_h_2  h1  h2  batch_size\n",
       "layer                                             \n",
       "1      171        25        15  69  42         512\n",
       "2       21         4         4  21  21         512\n",
       "3       47         1         4  46  46         512\n",
       "4       17         4         2  17  16         512\n",
       "5       18         3         3  18  18         512\n",
       "6       10         3         3  10  10         512\n",
       "7        9         1         2   9   9         512"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['h0', 'delta_h_1', 'delta_h_2', 'h1', 'h2']\n",
    "df = pd.DataFrame(columns=columns, index=range(1, L+2))\n",
    "df.index.name = 'layer'\n",
    "\n",
    "for l in df.index:\n",
    "    maxes = dict()\n",
    "    \n",
    "    _, maxes['h0'] = torch.max(h0[l] , dim=1)\n",
    "    _, maxes['delta_h_1'] = torch.max(delta_h_1[l] , dim=1)\n",
    "    _, maxes['delta_h_2'] = torch.max(delta_h_2[l] , dim=1)\n",
    "    _, maxes['h1'] = torch.max(h1[l] , dim=1)\n",
    "    _, maxes['h2'] = torch.max(h2[l] , dim=1)\n",
    "\n",
    "    df.loc[l, columns] = [maxes[key].unique().numel() for key in columns]\n",
    "    \n",
    "df.loc[:, 'batch_size'] = batch_size\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h0</th>\n",
       "      <th>delta_h_1</th>\n",
       "      <th>delta_h_2</th>\n",
       "      <th>h1</th>\n",
       "      <th>h2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.91465</td>\n",
       "      <td>0.902734</td>\n",
       "      <td>1.15864</td>\n",
       "      <td>1.28904</td>\n",
       "      <td>1.77124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.4183</td>\n",
       "      <td>0.00362996</td>\n",
       "      <td>0.0104477</td>\n",
       "      <td>2.41916</td>\n",
       "      <td>2.42351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.70222</td>\n",
       "      <td>0.00540379</td>\n",
       "      <td>0.0150112</td>\n",
       "      <td>2.70288</td>\n",
       "      <td>2.70762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.86657</td>\n",
       "      <td>0.0064816</td>\n",
       "      <td>0.0172351</td>\n",
       "      <td>2.86723</td>\n",
       "      <td>2.87247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.14571</td>\n",
       "      <td>0.00661806</td>\n",
       "      <td>0.0177092</td>\n",
       "      <td>3.14623</td>\n",
       "      <td>3.15094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.22414</td>\n",
       "      <td>0.00611874</td>\n",
       "      <td>0.0160097</td>\n",
       "      <td>3.2243</td>\n",
       "      <td>3.22706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0846157</td>\n",
       "      <td>0.00222558</td>\n",
       "      <td>0.00523002</td>\n",
       "      <td>0.0862315</td>\n",
       "      <td>0.0902302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              h0   delta_h_1   delta_h_2         h1         h2\n",
       "layer                                                         \n",
       "1        0.91465    0.902734     1.15864    1.28904    1.77124\n",
       "2         2.4183  0.00362996   0.0104477    2.41916    2.42351\n",
       "3        2.70222  0.00540379   0.0150112    2.70288    2.70762\n",
       "4        2.86657   0.0064816   0.0172351    2.86723    2.87247\n",
       "5        3.14571  0.00661806   0.0177092    3.14623    3.15094\n",
       "6        3.22414  0.00611874   0.0160097     3.2243    3.22706\n",
       "7      0.0846157  0.00222558  0.00523002  0.0862315  0.0902302"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['h0', 'delta_h_1', 'delta_h_2', 'h1', 'h2']\n",
    "df = pd.DataFrame(columns=columns, index=range(1, L+2))\n",
    "df.index.name = 'layer'\n",
    "for l in df.index:\n",
    "    df.loc[l, columns] = [h0[l][0, :].abs().mean().item(), delta_h_1[l][0, :].abs().mean().item(), \n",
    "                          delta_h_2[l][0, :].abs().mean().item(), h1[l][0, :].abs().mean().item(),  \n",
    "                          h2[l][0, :].abs().mean().item()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3994)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[2][0, :] * delta_h_2[2][0, :]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4199)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[3][0, :] * delta_h_2[3][0, :]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4131)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[4][0, :] * delta_h_2[4][0, :]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4055)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[5] * delta_h_2[5]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4199)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[6] * delta_h_2[6]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3000)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod = h1[7][132, :] * delta_h_2[7][132, :]\n",
    "(prod < 0).sum() / prod.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6000)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h1[7][1, :] < 0).sum() / h1[7][1, :].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
