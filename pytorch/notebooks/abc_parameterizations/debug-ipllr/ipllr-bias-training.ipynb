{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "NOTEBOOK_DIR = os.path.dirname(cwd)\n",
    "ROOT = os.path.dirname(os.path.dirname(os.path.dirname(NOTEBOOK_DIR)))\n",
    "\n",
    "FIGURES_DIR = os.path.join(ROOT, 'figures/abc_parameterizations/debug_ipllr_renorm/bias')\n",
    "CONFIG_PATH = os.path.join(ROOT, 'pytorch/configs/abc_parameterizations', 'fc_ipllr_mnist.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.tools import read_yaml, set_random_seeds\n",
    "from pytorch.configs.base import BaseConfig\n",
    "from pytorch.configs.model import ModelConfig\n",
    "from pytorch.models.abc_params.fully_connected.ipllr_bias import FcIPLLRBias\n",
    "from pytorch.models.abc_params.fully_connected.muP import FCmuP\n",
    "from pytorch.models.abc_params.fully_connected.ntk import FCNTK\n",
    "from pytorch.models.abc_params.fully_connected.standard_fc_ip import StandardFCIP\n",
    "from utils.dataset.mnist import load_data\n",
    "from utils.abc_params.debug_ipllr import *\n",
    "from utils.plot.abc_parameterizations.debug_ipllr import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load basic configuration and define variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 1\n",
    "SEED = 30\n",
    "L = 6\n",
    "width = 1024\n",
    "n_warmup_steps = 1\n",
    "batch_size = 512\n",
    "base_lr = 0.1\n",
    "n_steps = 150\n",
    "renorm_first = False\n",
    "scale_first_lr = False\n",
    "activation = 'gelu'\n",
    "\n",
    "set_random_seeds(SEED)  # set random seed for reproducibility\n",
    "config_dict = read_yaml(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = read_yaml(CONFIG_PATH)\n",
    "\n",
    "input_size = config_dict['architecture']['input_size']\n",
    "\n",
    "config_dict['architecture']['width'] = width\n",
    "config_dict['architecture']['n_layers'] = L + 1\n",
    "config_dict['optimizer']['params']['lr'] = base_lr\n",
    "config_dict['activation']['name'] = activation\n",
    "config_dict['scheduler'] = {'name': 'warmup_switch',\n",
    "                            'params': {'n_warmup_steps': n_warmup_steps,\n",
    "                                       'calibrate_base_lr': False,\n",
    "                                       'default_calibration': False}}\n",
    "        \n",
    "base_model_config = ModelConfig(config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data & define models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, test_dataset = load_data(download=False, flatten=True)\n",
    "train_data_loader = DataLoader(training_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_batches = list(DataLoader(test_dataset, shuffle=False, batch_size=batch_size))\n",
    "batches = list(train_data_loader)\n",
    "eval_batch = test_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard ipllr bias\n",
    "config_dict['scheduler']['params']['calibrate_base_lr'] = False\n",
    "config = ModelConfig(config_dict)\n",
    "ipllrs_bias = [FcIPLLRBias(config) for _ in range(N_TRIALS)]\n",
    "\n",
    "# ipllr bias with default calibration (i.e. base_lr = 1.0 for first step)\n",
    "config_dict['scheduler']['params']['calibrate_base_lr'] = True\n",
    "config_dict['scheduler']['params']['default_calibration'] = True\n",
    "config = ModelConfig(config_dict)\n",
    "ipllrs_bias_calib_default = [FcIPLLRBias(config) for _ in range(N_TRIALS)]\n",
    "\n",
    "# ipllr bias with scaling bias lr\n",
    "config_dict['scheduler']['params']['calibrate_base_lr'] = False\n",
    "config = ModelConfig(config_dict)\n",
    "ipllrs_bias_scale_bias_lr = [FcIPLLRBias(config) for _ in range(N_TRIALS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_TRIALS):    \n",
    "    ipllrs_bias_calib_default[i].copy_initial_params_from_model(ipllrs_bias[i])\n",
    "    ipllrs_bias_scale_bias_lr[i].copy_initial_params_from_model(ipllrs_bias[i])\n",
    "    \n",
    "    ipllrs_bias_calib_default[i].initialize_params()\n",
    "    ipllrs_bias_scale_bias_lr[i].initialize_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure calibration takes into account normalization\n",
    "\n",
    "#for ipllr in ipllrs_calib:    \n",
    "#    initial_base_lrs = ipllr.scheduler.calibrate_base_lr(ipllr, batches=batches, normalize_first=False)\n",
    "#    ipllr.scheduler._set_param_group_lrs(initial_base_lrs)\n",
    "    \n",
    "#for ipllr in ipllrs_calib_renorm:        \n",
    "#    initial_base_lrs = ipllr.scheduler.calibrate_base_lr(ipllr, batches=batches, normalize_first=True)\n",
    "#    ipllr.scheduler._set_param_group_lrs(initial_base_lrs)\n",
    "    \n",
    "#for ipllr in ipllrs_calib_renorm_scale_lr:            \n",
    "#    initial_base_lrs = ipllr.scheduler.calibrate_base_lr(ipllr, batches=batches, normalize_first=True)\n",
    "#    ipllr.scheduler._set_param_group_lrs(initial_base_lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale lr of first layer if needed\n",
    "\n",
    "#for ipllr in ipllrs_renorm_scale_lr:\n",
    "#    for i, param_group in enumerate(ipllr.optimizer.param_groups):\n",
    "#        if i == 0:\n",
    "#            param_group['lr'] = param_group['lr'] * (ipllr.d + 1)\n",
    "#    ipllr.scheduler.warm_lrs[0] = ipllr.scheduler.warm_lrs[0] * (ipllr.d + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "# without calibration\n",
    "#results['ipllr'] = [collect_training_losses(ipllrs[i], batches, n_steps, normalize_first=False) \n",
    "#                    for i in range(N_TRIALS)]\n",
    "\n",
    "#results['ipllr_renorm'] = [collect_training_losses(ipllrs_renorm[i], batches, n_steps, normalize_first=True)\n",
    "#                           for i in range(N_TRIALS)]\n",
    "\n",
    "#results['ipllr_renorm_scale_lr'] = [collect_training_losses(ipllrs_renorm_scale_lr[i], batches, n_steps, \n",
    "#                                                            normalize_first=True) \n",
    "#                                    for i in range(N_TRIALS)]\n",
    "\n",
    "# with calibration\n",
    "results['ipllr_bias'] = [collect_training_losses(ipllrs_bias[i], batches, n_steps, normalize_first=False)\n",
    "                         for i in range(N_TRIALS)]\n",
    "\n",
    "results['ipllr_bias_calib_default'] = [collect_training_losses(ipllrs_bias_calib_default[i], batches, n_steps, \n",
    "                                                               normalize_first=False)\n",
    "                                       for i in range(N_TRIALS)]\n",
    "\n",
    "results['ipllrs_bias_scale_bias_lr'] = \\\n",
    "    [collect_training_losses(ipllrs_bias_scale_bias_lr[i], batches, n_steps, normalize_first=False) \n",
    "     for i in range(N_TRIALS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = dict()\n",
    "for key, res in results.items():\n",
    "    losses[key] = [r[0] for r in res]\n",
    "    \n",
    "chis = dict()\n",
    "for key, res in results.items():\n",
    "    chis[key] = [r[1] for r in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipllrs_bias[0].activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipllrs_bias_calib_default[0].activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipllrs_bias_scale_bias_lr[0].activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'loss'\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_losses_models(losses, key=key, L=L, width=width, activation=activation, lr=base_lr, batch_size=batch_size, \n",
    "                   mode=mode, normalize_first=renorm_first, marker=None, name='IPLLR')\n",
    "#plt.savefig(\n",
    "#    os.path.join(FIGURES_DIR, 'IPLLRs_1_last_small_{}_{}_L={}_m={}_lr={}_bs={}.png'.\\\n",
    "#                 format(mode, key, L, width, base_lr, batch_size, renorm_first, scale_first_lr)))\n",
    "plt.ylim(0,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'chi'\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_losses_models(chis, key=key, L=L, width=width, activation=activation, lr=base_lr, batch_size=batch_size, \n",
    "                   mode=mode, marker=None, name='IPLLR-bias')\n",
    "#plt.savefig(os.path.join(FIGURES_DIR, 'IPLLRs_1_last_small_{}_{}_L={}_m={}_lr={}_bs={}.png'.\\\n",
    "#                         format(mode, key, L, width, base_lr, batch_size)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
