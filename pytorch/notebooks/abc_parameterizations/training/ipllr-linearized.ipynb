{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33a5f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "918c35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0748368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "NOTEBOOK_DIR = os.path.dirname(cwd)\n",
    "ROOT = os.path.dirname(os.path.dirname(os.path.dirname(NOTEBOOK_DIR)))\n",
    "\n",
    "FIGURES_DIR = os.path.join(ROOT, 'figures/abc_parameterizations/training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73eee4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b393b9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karlhajjar/Documents/projects/wide-networks/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as pylab\n",
    "from copy import deepcopy\n",
    "\n",
    "from pytorch.configs.model import ModelConfig\n",
    "from pytorch.models.abc_params.fully_connected.ipllr import FcIPLLR\n",
    "from utils.tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90c27f",
   "metadata": {},
   "source": [
    "## Set variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69e137e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 8\n",
    "WIDTH = 1024  # 8000\n",
    "BASE_LR = 1.0\n",
    "N_STEPS = 2\n",
    "BIAS = False\n",
    "ACTIVATION = 'sigmoid'\n",
    "CONFIG_FILE = 'fc_ipllr_mnist.yaml'\n",
    "\n",
    "DIM = 20\n",
    "OUTPUT_DIM = 1\n",
    "LOSS = 'mse'\n",
    "N_VAL = 100\n",
    "SEED = 42\n",
    "\n",
    "FONTSIZE = 12\n",
    "FIGSIZE = (10, 6)\n",
    "\n",
    "fig_dir = os.path.join(ROOT, FIGURES_DIR, 'linearization')\n",
    "\n",
    "params = {'legend.fontsize': FONTSIZE,\n",
    "         'axes.labelsize': FONTSIZE,\n",
    "         'axes.titlesize': FONTSIZE,\n",
    "         'xtick.labelsize': FONTSIZE,\n",
    "         'ytick.labelsize': FONTSIZE}\n",
    "pylab.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71fce2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_random_seeds(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbef40b",
   "metadata": {},
   "source": [
    "## Model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b5f3612",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.randn(size=(N_STEPS+1, 1, DIM), requires_grad=False)\n",
    "y_train = torch.ones(size=(N_STEPS+1, 1, OUTPUT_DIM), requires_grad=False) / 2\n",
    "\n",
    "x_val = torch.randn(size=(N_STEPS, 1, DIM), requires_grad=False)\n",
    "y_val = torch.ones(size=(N_STEPS, 1, OUTPUT_DIM), requires_grad=False) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4983b141",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = read_yaml(os.path.join(ROOT, 'pytorch/configs/abc_parameterizations', CONFIG_FILE))\n",
    "config_dict['architecture']['width'] = WIDTH\n",
    "config_dict['architecture']['n_layers'] = L + 1\n",
    "config_dict['architecture']['input_size'] = DIM\n",
    "config_dict['architecture']['output_size'] = OUTPUT_DIM\n",
    "config_dict['optimizer']['params']['lr'] = BASE_LR\n",
    "config_dict['activation']['name'] = ACTIVATION\n",
    "config_dict['loss'] = {'name': 'mse', 'params': {'reduction': 'mean'}}\n",
    "config_dict['scheduler']['params']['calibrate_base_lr'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac0902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(config_dict=config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "469b7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FcIPLLR(config)\n",
    "pg = list(model.optimizer.param_groups)\n",
    "pg[0]['lr'] = pg[0]['lr'] / DIM\n",
    "#for l in range(2, L):\n",
    "#    pg[l]['lr'] = pg[l]['lr'] * (WIDTH ** 0.5)\n",
    "model_0 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ebe89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(x, y):\n",
    "    h_grads = []\n",
    "    x_grads = []\n",
    "\n",
    "    hs = []\n",
    "    xs = []\n",
    "    \n",
    "    model.optimizer.zero_grad()\n",
    "    h = (model.width ** (-model.a[0])) * model.input_layer.forward(x)  # h_0 first layer pre-activations\n",
    "    hs.append(h)\n",
    "\n",
    "    x = model.activation(h)  # x_0, first layer activations\n",
    "    xs.append(x)\n",
    "\n",
    "    for l, layer in enumerate(model.intermediate_layers):  # L-1 intermediate layers\n",
    "        h = (model.width ** (-model.a[l+1])) * layer.forward(x)  # h_l, layer l pre-activations\n",
    "        hs.append(h)\n",
    "        x = model.activation(h)  # x_l, l-th layer activations\n",
    "        xs.append(x)\n",
    "    \n",
    "    for h_ in hs:\n",
    "        h_.retain_grad()\n",
    "    for x_ in xs:\n",
    "        x_.retain_grad()\n",
    "        \n",
    "    y_hat = (model.width ** (-model.a[model.n_layers-1])) * model.output_layer.forward(x)  # f(x)\n",
    "    y_hat.retain_grad()\n",
    "    \n",
    "    loss_ = model.loss(y_hat, y)\n",
    "    loss_.backward()\n",
    "    \n",
    "    h_grads = [h_.grad for h_ in hs]\n",
    "    x_grads = [x_.grad for x_ in xs]\n",
    "    \n",
    "    y_hat_grad = y_hat.grad\n",
    "    \n",
    "    return  hs, xs, y_hat, h_grads, x_grads, y_hat_grad, loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f17733",
   "metadata": {},
   "source": [
    "## 1st forward backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "094ab80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs0, xs0, y_hat0, h_grads0, x_grads0, y_hat_grad0, loss_0 = forward_backward(x_train[0, : ,:], y_train[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5551d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tilde_hs0 = []\n",
    "tilde_xs0 = []\n",
    "tilde_h_grads0 = []\n",
    "tilde_x_grads0 = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for l in range(L):\n",
    "        forward_scale = WIDTH ** (l/2)\n",
    "        tilde_hs0.append(forward_scale * hs0[l])\n",
    "        tilde_xs0.append(forward_scale * xs0[l])\n",
    "        \n",
    "        backward_scale = WIDTH * (WIDTH ** ((L-(l+1)) / 2))\n",
    "        tilde_h_grads0.append(backward_scale * h_grads0[l])\n",
    "        tilde_x_grads0.append(backward_scale * x_grads0[l])\n",
    "        \n",
    "    tilde_y_hat0 = (WIDTH ** (L/2)) * y_hat0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "586d14ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.698747634887695\n",
      "0.7188931107521057\n",
      "482.9482727050781\n",
      "518127.65625\n",
      "546759232.0\n",
      "556967133184.0\n",
      "540849393893376.0\n",
      "5.763604562271273e+17\n",
      "\n",
      "0.34303274750709534\n",
      "256.25225830078125\n",
      "262388.6875\n",
      "268436192.0\n",
      "274880626688.0\n",
      "281318512394240.0\n",
      "2.8809505032214938e+17\n",
      "2.953098588440777e+20\n",
      "4.394934928351072e+20\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for tilde_h in tilde_hs0:\n",
    "        print(torch.sum((tilde_h)**2).detach().item() / WIDTH)\n",
    "    print('')\n",
    "\n",
    "    for tilde_x in tilde_xs0:\n",
    "        print(torch.sum((tilde_x)**2).detach().item() / WIDTH)\n",
    "        \n",
    "    print((tilde_y_hat0.detach().item())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19df08de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0266147043580531e-08\n",
      "1.8961415548801597e-07\n",
      "1.6430534515166073e-06\n",
      "1.2515798516687937e-05\n",
      "9.844550368143246e-05\n",
      "0.0008299332694150507\n",
      "0.006708770990371704\n",
      "0.05289517343044281\n",
      "\n",
      "3.5941593523602933e-07\n",
      "3.0349037842825055e-06\n",
      "2.6294979761587456e-05\n",
      "0.00020029988081660122\n",
      "0.001575519680045545\n",
      "0.013282040134072304\n",
      "0.10736584663391113\n",
      "0.8465492725372314\n",
      "0.9251871599622206\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for tilde_h_grad in tilde_h_grads0:\n",
    "        print(torch.sum((tilde_h_grad)**2).detach().item() / WIDTH)\n",
    "    print('')\n",
    "\n",
    "    for tilde_x_grad in tilde_x_grads0:\n",
    "        print(torch.sum((tilde_x_grad)**2).detach().item() / WIDTH)\n",
    "        \n",
    "    print((y_hat_grad0.detach().item())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c91b6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1676594831494396\n",
      "0.026496113931458953\n",
      "0.021461019636345207\n",
      "0.021966883775315825\n",
      "0.022299655258533377\n",
      "0.022241541946912635\n",
      "0.02165899513544738\n",
      "0.022095164537040607\n",
      "\n",
      "0.5856899755904103\n",
      "0.5002462853403878\n",
      "0.5002332977564525\n",
      "0.5000006854529451\n",
      "0.5000024735866395\n",
      "0.499861012453168\n",
      "0.4998826097728106\n",
      "0.5001371612767347\n",
      "0.019066737964749336\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for h in hs0:\n",
    "        print(np.sqrt(torch.sum((h)**2).detach().item() / WIDTH))\n",
    "    print('')\n",
    "\n",
    "    for x in xs0:\n",
    "        print(np.sqrt(torch.sum((x)**2).detach().item() / WIDTH))\n",
    "        \n",
    "    print(y_hat0.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd57358",
   "metadata": {},
   "source": [
    "## 1st optimizer step : weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9550822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.step()\n",
    "model.scheduler.step()\n",
    "model_1 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58b1809",
   "metadata": {},
   "source": [
    "## 2nd forward backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9dd3cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs1, xs1, y_hat1, h_grads1, x_grads1, y_hat_grad1, loss_1 = forward_backward(x_train[1, : ,:], y_train[1, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ffd346a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9267763735326202\n",
      "0.025843651392611935\n",
      "0.024114697395927604\n",
      "0.9075732272005221\n",
      "80.9285709360421\n",
      "7664.12865236486\n",
      "669404.6828160078\n",
      "60855934.83142455\n",
      "\n",
      "0.5739621737490722\n",
      "0.500207828006174\n",
      "0.5003017468699691\n",
      "0.5346124210853432\n",
      "0.7091570767323151\n",
      "0.6980721219902711\n",
      "0.7022564079451322\n",
      "0.7173900177030623\n",
      "8508037120.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for h in hs1:\n",
    "        print(np.sqrt(torch.sum((h)**2).detach().item() / WIDTH))\n",
    "    print('')\n",
    "\n",
    "    for x in xs1:\n",
    "        print(np.sqrt(torch.sum((x)**2).detach().item() / WIDTH))\n",
    "        \n",
    "    print(y_hat1.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b12de84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2810658812522888\n",
      "8.00703239440918\n",
      "256.2706298828125\n",
      "8155.666015625\n",
      "266035.875\n",
      "8172732.0\n",
      "264603104.0\n",
      "8845340672.0\n",
      "0.019277792423963547\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for l in range(L):\n",
    "        print(torch.sum(tilde_xs0[l] * xs1[l]).detach().item() / WIDTH)\n",
    "    print(model_0.output_layer(xs1[2]).detach().item() / WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b061d350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2810658812522888\n",
      "256.22503662109375\n",
      "8200.66015625\n",
      "260981.3125\n",
      "8513148.0\n",
      "261527424.0\n",
      "8467299328.0\n",
      "283050901504.0\n",
      "0.6168893575668335\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for l in range(L):\n",
    "        if l == 0:\n",
    "            print(torch.sum(tilde_xs0[l] * xs1[l]).detach().item() / WIDTH)\n",
    "        else:\n",
    "            print(torch.sum(tilde_xs0[l] * xs1[l]).detach().item() / np.sqrt(WIDTH))\n",
    "    print(model_0.output_layer(xs1[2]).detach().item() / np.sqrt(WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be6e72ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2810658812522888\n",
      "256.22503662109375\n",
      "8200.66015625\n",
      "0.6168893575668335\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.sum(tilde_xs0[0] * xs1[0]).detach().item() / WIDTH)\n",
    "    print(torch.sum(tilde_xs0[1] * xs1[1]).detach().item() / np.sqrt(WIDTH))\n",
    "    print(torch.sum(tilde_xs0[2] * xs1[2]).detach().item() / np.sqrt(WIDTH))\n",
    "    \n",
    "    print(model_0.output_layer(xs1[2]).detach().item() / np.sqrt(WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8ae9c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "2.746712305533744e+17\n",
      "17016074240.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for h_grad in h_grads1:\n",
    "        print(np.sqrt(torch.sum((h_grad)**2).detach().item() / WIDTH))\n",
    "    print('')\n",
    "\n",
    "    for x_grad in x_grads1:\n",
    "        print(np.sqrt(torch.sum((x_grad)**2).detach().item() / WIDTH))\n",
    "        \n",
    "    print(y_hat_grad1.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c6bd80",
   "metadata": {},
   "source": [
    "## 2nd optimizer step : weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a3fdc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1024.0, 1048576.0, 1048576.0, 1048576.0, 1048576.0, 1048576.0, 1048576.0, 1048576.0, 1024.0]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print([g['lr'] for g in model.optimizer.param_groups])\n",
    "    #for g in model.optimizer.param_groups:\n",
    "    #    g['lr'] = g['lr'] * 0.0001\n",
    "    #print([g['lr'] for g in model.optimizer.param_groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce837fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.step()\n",
    "model_2 = deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a426af6",
   "metadata": {},
   "source": [
    "## 3rd forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1cd8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs2, xs2, y_hat2, h_grads2, x_grads2, y_hat_grad2, loss_2 = forward_backward(x_train[2, : ,:], y_train[2, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61d77d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9329723245817247\n",
      "0.02653240171056875\n",
      "0.024112040880760945\n",
      "0.9075758213494385\n",
      "80.92856791929998\n",
      "7664.12865236486\n",
      "669404.6828160078\n",
      "60855934.83142455\n",
      "\n",
      "0.58707340762283\n",
      "0.5002197736294834\n",
      "0.5003032063008787\n",
      "0.5346125604494756\n",
      "0.7091570767323151\n",
      "0.6980721219902711\n",
      "0.7022564079451322\n",
      "0.7173900177030623\n",
      "-249259392.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for h in hs2:\n",
    "        print(np.sqrt(torch.sum((h)**2).detach().item() / WIDTH))\n",
    "    print('')\n",
    "\n",
    "    for x in xs2:\n",
    "        print(np.sqrt(torch.sum((x)**2).detach().item() / WIDTH))\n",
    "        \n",
    "    print(y_hat2.detach().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b61c7d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2872248888015747\n",
      "8.007235527038574\n",
      "256.27142333984375\n",
      "8155.666015625\n",
      "266035.875\n",
      "8172732.0\n",
      "264603104.0\n",
      "8845340672.0\n",
      "0.019279837608337402\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for l in range(L):\n",
    "        print(torch.sum(tilde_xs0[l] * xs2[l]).detach().item() / WIDTH)\n",
    "\n",
    "    \n",
    "    print(model_0.output_layer(xs2[2]).detach().item() / WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a59b2990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2874434292316437\n",
      "0.2502078115940094\n",
      "0.2503025531768799\n",
      "0.28581053018569946\n",
      "0.5029037594795227\n",
      "0.4873046875\n",
      "0.4931640625\n",
      "0.5146484375\n",
      "8268525568.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for l in range(L):\n",
    "        print(torch.sum(xs1[l] * xs2[l]).detach().item() / WIDTH)\n",
    "    \n",
    "    print(model_1.output_layer(xs2[2]).detach().item() / WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5bb223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
